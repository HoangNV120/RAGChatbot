"""
ÄÃ¡nh giÃ¡ RAG Chat sá»­ dá»¥ng RAGAS
ÄÃ¡nh giÃ¡ cÃ¡c tiÃªu chÃ­: Faithfulness, Answer Relevancy, Answer Correctness
"""

import asyncio
import pandas as pd
from typing import List, Dict, Any
import os
import sys
from dotenv import load_dotenv
import json

# ThÃªm thÆ° má»¥c gá»‘c vÃ o Python path Ä‘á»ƒ import cÃ¡c module RAG
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import RAGAS
from ragas import evaluate, EvaluationDataset
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness,
    answer_similarity,

    Faithfulness,
    AnswerRelevancy,
    ContextPrecision,
    ContextRecall,
    AnswerCorrectness,
    AnswerSimilarity,
)
# Import LLM configuration
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Import cÃ¡c module RAG
from app.rag_chat import RAGChat
from app.vector_store import VectorStore
from app.document_processor import DocumentProcessor
from app.safety_guard import SafetyGuard
from app.config import settings

load_dotenv()

class RAGEvaluator:
    def __init__(self):
        """Khá»Ÿi táº¡o evaluator vá»›i RAG components"""
        self.rag_chat = None
        self.vector_store = None
        self.doc_processor = None
        self.safety_guard = None
        self.llm = None
        self.embeddings = None

        # Khá»Ÿi táº¡o LLM vÃ  embeddings tá»« config
        self._setup_llm_and_embeddings()

    def _setup_llm_and_embeddings(self):
        """Thiáº¿t láº­p LLM vÃ  embeddings tá»« config"""
        try:
            # Khá»Ÿi táº¡o LLM tá»« config settings
            self.llm = ChatOpenAI(
                model=settings.model_name,
                temperature=settings.temperature,
                api_key=settings.openai_api_key
            )

            # Khá»Ÿi táº¡o embeddings tá»« config settings
            self.embeddings = OpenAIEmbeddings(
                model=settings.embedding_model,
                api_key=settings.openai_api_key
            )

            print(f"âœ… ÄÃ£ thiáº¿t láº­p LLM: {settings.model_name}")
            print(f"âœ… ÄÃ£ thiáº¿t láº­p Embeddings: {settings.embedding_model}")

        except Exception as e:
            print(f"âŒ Lá»—i khi thiáº¿t láº­p LLM/Embeddings: {e}")
            raise

    async def setup_rag_components(self):
        """Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n RAG"""
        print("ğŸš€ Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n RAG...")

        # Khá»Ÿi táº¡o document processor - sá»­a Ä‘Æ°á»ng dáº«n tá»« evaluation folder
        data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "app", "data")
        self.doc_processor = DocumentProcessor(data_dir=data_dir)

        # Khá»Ÿi táº¡o RAG chat
        self.rag_chat = RAGChat(vector_store=self.doc_processor.vector_store)

        # # Khá»Ÿi táº¡o Safety Guard
        # self.safety_guard = SafetyGuard()
        # print("âœ… Safety Guard Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o")

        # Load documents náº¿u chÆ°a load
        await self.doc_processor.load_and_process_all()
        print("âœ… RAG components Ä‘Ã£ sáºµn sÃ ng!")

    def load_test_data(self, excel_path: str) -> pd.DataFrame:
        """
        Load dá»¯ liá»‡u test tá»« file Excel
        Expected format: columns ['question', 'answer']
        """
        try:
            df = pd.read_excel(excel_path)
            print(f"ğŸ“Š ÄÃ£ load {len(df)} test cases tá»« {excel_path}")

            # Kiá»ƒm tra format
            required_columns = ['question', 'answer']
            for col in required_columns:
                if col not in df.columns:
                    raise ValueError(f"Missing required column: {col}")

            return df

        except Exception as e:
            print(f"âŒ Lá»—i khi load file Excel: {e}")
            raise

    async def get_rag_response_with_context(self, question: str) -> Dict[str, Any]:
        """
        Láº¥y response vÃ  context tá»« RAG system vá»›i Safety Guard
        """
        try:
            # # ğŸ›¡ï¸ Kiá»ƒm tra Safety Guard trÆ°á»›c khi vÃ o RAG
            # print(f"ğŸ” Kiá»ƒm tra safety cho cÃ¢u há»i: {question[:50]}...")
            # safety_result = await self.safety_guard.check_safety(question)
            #
            # if not safety_result["is_safe"]:
            #     # Náº¿u khÃ´ng an toÃ n, tráº£ vá» response tá»« safety guard
            #     safety_reason = safety_result["reason"]
            #     # Äáº£m báº£o safety_reason lÃ  string, khÃ´ng pháº£i None hoáº·c kiá»ƒu khÃ¡c
            #     if safety_reason is None:
            #         safety_reason = "CÃ¢u há»i khÃ´ng phÃ¹ há»£p vá»›i há»‡ thá»‘ng."
            #     elif not isinstance(safety_reason, str):
            #         safety_reason = str(safety_reason)
            #
            #     print(f"âš ï¸ CÃ¢u há»i bá»‹ tá»« chá»‘i bá»Ÿi Safety Guard: {safety_reason}")
            #     return {
            #         "contexts": ["[SAFETY BLOCKED] " + str(safety_reason)],  # luÃ´n lÃ  list[str]
            #         "answer": str(safety_reason),                             # luÃ´n lÃ  string
            #         "safety_status": "blocked"
            #     }
            #
            # print("âœ… CÃ¢u há»i Ä‘Ã£ qua kiá»ƒm tra safety, tiáº¿p tá»¥c vá»›i RAG...")

            # Láº¥y documents liÃªn quan (context)
            docs = await self.rag_chat.vector_store.similarity_search(question, k=4)

            # Táº¡o context tá»« documents
            contexts = []
            if docs:
                for doc in docs:
                    contexts.append(doc.page_content)
            else:
                contexts = ["KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."]

            # Láº¥y response tá»« RAG
            rag_result = await self.rag_chat.generate_response(question)
            actual_output = rag_result.get("output", "")

            # Äáº£m báº£o actual_output lÃ  string
            if not isinstance(actual_output, str):
                actual_output = str(actual_output)

            return {
                "contexts": contexts,  # List of strings
                "answer": actual_output,  # String
                "safety_status": "approved"
            }

        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y RAG response: {e}")
            return {
                "contexts": ["ÄÃ£ xáº£y ra lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½"],  # List of strings
                "answer": f"Lá»—i: {str(e)}",  # String
                "safety_status": "error"
            }

    async def create_evaluation_dataset(self, df: pd.DataFrame) -> List[SingleTurnSample]:
        """
        Táº¡o dataset cho RAGAS tá»« DataFrame
        """
        samples = []

        print("ğŸ”„ Táº¡o evaluation dataset...")
        for idx, row in df.iterrows():
            question = str(row['question'])
            ground_truth = str(row['answer'])

            print(f"Xá»­ lÃ½ test case {idx + 1}/{len(df)}: {question[:50]}...")

            # Láº¥y response vÃ  context tá»« RAG
            rag_result = await self.get_rag_response_with_context(question)

            # Táº¡o sample theo format RAGAS SingleTurnSample object
            sample = SingleTurnSample(
                user_input=question,
                response=rag_result["answer"],
                retrieved_contexts=rag_result["contexts"],
                reference=ground_truth
            )
            samples.append(sample)

        print(f"âœ… ÄÃ£ táº¡o dataset vá»›i {len(samples)} test cases")

        # Tráº£ vá» list cÃ¡c SingleTurnSample objects cho RAGAS
        return samples

    def setup_metrics(self) -> List:
        """
        Thiáº¿t láº­p cÃ¡c metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ vá»›i RAGAS sá»­ dá»¥ng LLM tá»« config
        """
        # Cáº¥u hÃ¬nh metrics vá»›i LLM tá»« config
        print(f"ğŸ”§ Cáº¥u hÃ¬nh RAGAS metrics vá»›i LLM: {settings.model_name}")

        # Import Ä‘á»ƒ cáº¥u hÃ¬nh LLM cho RAGAS
        from ragas.llms import LangchainLLMWrapper
        from ragas.embeddings import LangchainEmbeddingsWrapper

        # Táº¡o RAGAS LLM vÃ  embeddings wrappers
        ragas_llm = LangchainLLMWrapper(self.llm)
        ragas_embeddings = LangchainEmbeddingsWrapper(self.embeddings)

        # Cáº¥u hÃ¬nh metrics vá»›i LLM tÃ¹y chá»‰nh
        metrics = [
            faithfulness,
            answer_correctness,
            answer_similarity
        ]

        print(f"âœ… ÄÃ£ cáº¥u hÃ¬nh {len(metrics)} metrics vá»›i LLM tÃ¹y chá»‰nh")
        return metrics

    async def run_evaluation(self, excel_path: str, output_path: str = "ragas_evaluation_results.json"):
        """
        Cháº¡y Ä‘Ã¡nh giÃ¡ hoÃ n chá»‰nh vá»›i RAGAS
        """
        try:
            # Setup RAG components
            await self.setup_rag_components()

            # Load test data
            df = self.load_test_data(excel_path)

            # Create evaluation dataset - nháº­n list of samples
            samples = await self.create_evaluation_dataset(df)

            # Convert to Dataset sá»­ dá»¥ng constructor vá»›i samples
            dataset = EvaluationDataset(samples=samples)

            # Setup metrics
            metrics = self.setup_metrics()

            print("ğŸ¯ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ vá»›i RAGAS...")
            print(f"ğŸ¤– Sá»­ dá»¥ng LLM: {settings.model_name}")
            print(f"ğŸ” Sá»­ dá»¥ng Embeddings: {settings.embedding_model}")

            # Run evaluation
            result = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=self.llm,
                embeddings=self.embeddings,
            )

            # Save results - táº¡o dataset_dict Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i save method
            dataset_dict = {
                "question": [sample.user_input for sample in samples],
                "answer": [sample.response for sample in samples],
                "contexts": [sample.retrieved_contexts for sample in samples],
                "ground_truth": [sample.reference for sample in samples]
            }

            self.save_detailed_results(result, dataset_dict, output_path)

            print(f"âœ… ÄÃ¡nh giÃ¡ hoÃ n thÃ nh! Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")

            return result

        except Exception as e:
            print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡: {e}")
            raise

    def save_detailed_results(self, evaluation_result, dataset_dict: Dict, output_path: str):
        """
        LÆ°u káº¿t quáº£ chi tiáº¿t vÃ o file
        """
        # Debug: In ra cáº¥u trÃºc cá»§a evaluation_result Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n
        print(f"ğŸ” Debug - Type cá»§a evaluation_result: {type(evaluation_result)}")

        # Táº¡o káº¿t quáº£ chi tiáº¿t cho tá»«ng test case
        detailed_results = []

        for i in range(len(dataset_dict["question"])):
            result = {
                "test_case_id": i + 1,
                "question": dataset_dict["question"][i],
                "answer": dataset_dict["answer"][i],
                "ground_truth": dataset_dict["ground_truth"][i],
                "contexts": dataset_dict["contexts"][i],
                "metrics_scores": {}
            }

            # Thá»­ cÃ¡c cÃ¡ch khÃ¡c nhau Ä‘á»ƒ truy cáº­p scores
            try:
                # CÃ¡ch 1: Náº¿u lÃ  DataFrame (phá»• biáº¿n vá»›i RAGAS má»›i)
                if hasattr(evaluation_result, 'to_pandas'):
                    df = evaluation_result.to_pandas()
                    if i < len(df):
                        for col in df.columns:
                            if col not in ['question', 'answer', 'contexts', 'ground_truth', 'user_input', 'retrieved_contexts', 'response', 'reference']:
                                result["metrics_scores"][col] = float(df.iloc[i][col]) if pd.notna(df.iloc[i][col]) else 0.0

                # CÃ¡ch 2: Náº¿u cÃ³ attribute scores
                elif hasattr(evaluation_result, 'scores'):
                    scores_dict = evaluation_result.scores
                    for metric_name, score_list in scores_dict.items():
                        if isinstance(score_list, list) and i < len(score_list):
                            result["metrics_scores"][metric_name] = float(score_list[i]) if score_list[i] is not None else 0.0
                        else:
                            result["metrics_scores"][metric_name] = float(score_list) if score_list is not None else 0.0

                # CÃ¡ch 3: Náº¿u lÃ  dictionary trá»±c tiáº¿p (trÆ°á»ng há»£p nÃ y)
                elif isinstance(evaluation_result, dict):
                    for metric_name, score_list in evaluation_result.items():
                        if isinstance(score_list, list) and i < len(score_list):
                            result["metrics_scores"][metric_name] = float(score_list[i]) if score_list[i] is not None else 0.0
                        else:
                            result["metrics_scores"][metric_name] = float(score_list) if score_list is not None else 0.0

                # CÃ¡ch 4: Truy cáº­p trá»±c tiáº¿p cÃ¡c attributes
                else:
                    for metric_name in ['faithfulness', 'answer_relevancy', 'answer_correctness',
                                      'context_precision', 'context_recall', 'answer_similarity']:
                        if hasattr(evaluation_result, metric_name):
                            score_data = getattr(evaluation_result, metric_name)
                            if isinstance(score_data, list) and i < len(score_data):
                                result["metrics_scores"][metric_name] = float(score_data[i]) if score_data[i] is not None else 0.0
                            else:
                                result["metrics_scores"][metric_name] = float(score_data) if score_data is not None else 0.0

            except Exception as e:
                print(f"âš ï¸ Lá»—i khi truy cáº­p scores cho test case {i+1}: {e}")

            detailed_results.append(result)

        # Convert evaluation_result Ä‘á»ƒ láº¥y raw data trÆ°á»›c
        raw_data = self._convert_evaluation_results_to_dict(evaluation_result)

        # Náº¿u metrics_scores váº«n trá»‘ng, thá»­ extract tá»« raw data
        if detailed_results and not detailed_results[0]["metrics_scores"]:
            print("ğŸ”„ Metrics scores trá»‘ng, thá»­ extract tá»« raw data...")
            if isinstance(raw_data, list):
                for i, raw_item in enumerate(raw_data):
                    if i < len(detailed_results):
                        # Extract scores tá»« raw data
                        for metric_name in ['faithfulness', 'answer_relevancy', 'answer_correctness',
                                          'context_precision', 'context_recall', 'answer_similarity']:
                            if metric_name in raw_item:
                                score = raw_item[metric_name]
                                detailed_results[i]["metrics_scores"][metric_name] = float(score) if score is not None else 0.0

        # Táº¡o summary
        summary = {
            "total_test_cases": len(dataset_dict["question"]),
            "average_scores": {},
            "overall_performance": {},
            "evaluation_config": {
                "llm_model": settings.model_name,
                "embedding_model": settings.embedding_model,
                "temperature": settings.temperature
            }
        }

        # TÃ­nh Ä‘iá»ƒm trung bÃ¬nh cho tá»«ng metric
        all_metrics = set()
        for detail_result in detailed_results:
            all_metrics.update(detail_result["metrics_scores"].keys())

        for metric_name in all_metrics:
            scores = []
            for detail_result in detailed_results:
                if metric_name in detail_result["metrics_scores"]:
                    scores.append(detail_result["metrics_scores"][metric_name])

            avg_score = sum(scores) / len(scores) if scores else 0
            summary["average_scores"][metric_name] = avg_score

        # ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ
        avg_faithfulness = summary["average_scores"].get("faithfulness", 0)
        avg_relevancy = summary["average_scores"].get("answer_relevancy", 0)
        avg_correctness = summary["average_scores"].get("answer_correctness", 0)

        overall_score = (avg_faithfulness + avg_relevancy + avg_correctness) / 3
        summary["overall_performance"]["overall_score"] = overall_score

        if overall_score >= 0.8:
            summary["overall_performance"]["grade"] = "Excellent"
        elif overall_score >= 0.7:
            summary["overall_performance"]["grade"] = "Good"
        elif overall_score >= 0.6:
            summary["overall_performance"]["grade"] = "Fair"
        else:
            summary["overall_performance"]["grade"] = "Needs Improvement"

        # LÆ°u káº¿t quáº£
        final_results = {
            "summary": summary,
            "detailed_results": detailed_results,
            "raw_evaluation_result": raw_data
        }

        # LÆ°u vÃ o file JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)

        # Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t
        self.create_summary_report(summary, detailed_results, output_path.replace('.json', '_summary.txt'))

        # Debug: In ra má»™t sá»‘ káº¿t quáº£ Ä‘á»ƒ kiá»ƒm tra
        print(f"ğŸ” Debug - Sá»‘ metrics tÃ¬m Ä‘Æ°á»£c: {len(all_metrics)}")
        print(f"ğŸ” Debug - Metrics: {list(all_metrics)}")
        if detailed_results:
            print(f"ğŸ” Debug - Scores cá»§a test case Ä‘áº§u tiÃªn: {detailed_results[0]['metrics_scores']}")

    def _convert_evaluation_results_to_dict(self, evaluation_result):
        """
        Chuyá»ƒn Ä‘á»•i evaluation result thÃ nh dictionary Ä‘á»ƒ lÆ°u vÃ o JSON
        """
        try:
            # Thá»­ convert pandas DataFrame
            if hasattr(evaluation_result, 'to_pandas'):
                df = evaluation_result.to_pandas()
                return df.to_dict('records')

            # Thá»­ convert dictionary
            elif isinstance(evaluation_result, dict):
                result_dict = {}
                for key, value in evaluation_result.items():
                    if hasattr(value, 'tolist'):
                        result_dict[key] = value.tolist()
                    elif isinstance(value, (int, float, str, bool, list, dict)):
                        result_dict[key] = value
                    else:
                        result_dict[key] = str(value)
                return result_dict

            # Fallback: truy cáº­p attributes
            else:
                result_dict = {}
                for attr in dir(evaluation_result):
                    if not attr.startswith('_') and not callable(getattr(evaluation_result, attr)):
                        try:
                            value = getattr(evaluation_result, attr)
                            if hasattr(value, 'tolist'):
                                result_dict[attr] = value.tolist()
                            elif isinstance(value, (int, float, str, bool, list, dict)):
                                result_dict[attr] = value
                            else:
                                result_dict[attr] = str(value)
                        except:
                            continue
                return result_dict

        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i evaluation result: {e}")
            return {"error": str(e), "type": str(type(evaluation_result))}

    def create_summary_report(self, summary: Dict, detailed_results: List[Dict], summary_path: str):
        """
        Táº¡o bÃ¡o cÃ¡o tá»•ng káº¿t dáº¡ng text
        """
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("BÃO CÃO ÄÃNH GIÃ RAG SYSTEM Vá»šI RAGAS\n")
            f.write("="*60 + "\n\n")

            # ThÃ´ng tin tá»•ng quan
            f.write(f"Tá»•ng sá»‘ test cases: {summary['total_test_cases']}\n")
            f.write(f"Äiá»ƒm tá»•ng thá»ƒ: {summary['overall_performance']['overall_score']:.3f}\n")
            f.write(f"Xáº¿p loáº¡i: {summary['overall_performance']['grade']}\n\n")

            # Äiá»ƒm trung bÃ¬nh cÃ¡c metrics
            f.write("ÄIá»‚M TRUNG BÃŒNH CÃC METRICS:\n")
            f.write("-"*40 + "\n")
            for metric_name, score in summary['average_scores'].items():
                f.write(f"{metric_name:20}: {score:.3f}\n")

            f.write("\n" + "="*60 + "\n")
            f.write("CHI TIáº¾T Tá»ªNG TEST CASE:\n")
            f.write("="*60 + "\n\n")

            # Chi tiáº¿t tá»«ng test case
            for result in detailed_results:
                f.write(f"Test Case {result['test_case_id']}:\n")
                f.write(f"Question: {result['question'][:100]}...\n")
                f.write(f"Metrics scores:\n")
                for metric_name, score in result['metrics_scores'].items():
                    f.write(f"  - {metric_name}: {score:.3f}\n")
                f.write("\n")

        print(f"ğŸ“„ BÃ¡o cÃ¡o tÃ³m táº¯t Ä‘Æ°á»£c lÆ°u táº¡i: {summary_path}")

    def print_evaluation_summary(self, result):
        """
        In tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ra console
        """
        print("\n" + "="*60)
        print("ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ RAG SYSTEM Vá»šI RAGAS")
        print("="*60)

        try:
            # Thá»­ cÃ¡c cÃ¡ch khÃ¡c nhau Ä‘á»ƒ láº¥y scores
            scores_dict = {}

            # CÃ¡ch 1: DataFrame
            if hasattr(result, 'to_pandas'):
                df = result.to_pandas()
                for col in df.columns:
                    if col not in ['question', 'answer', 'contexts', 'ground_truth', 'user_input', 'retrieved_contexts', 'response', 'reference']:
                        scores = df[col].dropna().tolist()
                        if scores:
                            avg_score = sum(scores) / len(scores)
                            scores_dict[col] = avg_score

            # CÃ¡ch 2: Dictionary hoáº·c scores attribute
            elif hasattr(result, 'scores') or isinstance(result, dict):
                data = result.scores if hasattr(result, 'scores') else result
                for metric_name, score_data in data.items():
                    if isinstance(score_data, list):
                        valid_scores = [s for s in score_data if s is not None]
                        if valid_scores:
                            scores_dict[metric_name] = sum(valid_scores) / len(valid_scores)
                    else:
                        scores_dict[metric_name] = float(score_data) if score_data is not None else 0.0

            # CÃ¡ch 3: Attributes trá»±c tiáº¿p
            else:
                for metric_name in ['faithfulness', 'answer_relevancy', 'answer_correctness',
                                  'context_precision', 'context_recall', 'semantic_similarity']:
                    if hasattr(result, metric_name):
                        score_data = getattr(result, metric_name)
                        if isinstance(score_data, list):
                            valid_scores = [s for s in score_data if s is not None]
                            if valid_scores:
                                scores_dict[metric_name] = sum(valid_scores) / len(valid_scores)
                        else:
                            scores_dict[metric_name] = float(score_data) if score_data is not None else 0.0

            # Náº¿u váº«n khÃ´ng cÃ³ scores, thá»­ convert vÃ  extract
            if not scores_dict:
                print("ğŸ”„ KhÃ´ng tÃ¬m tháº¥y scores, thá»­ convert result...")
                converted = self._convert_evaluation_results_to_dict(result)
                if isinstance(converted, list):
                    # TÃ­nh trung bÃ¬nh tá»« converted data
                    metrics_sums = {}
                    metrics_counts = {}

                    for item in converted:
                        for metric_name in ['faithfulness', 'answer_relevancy', 'answer_correctness',
                                          'context_precision', 'context_recall', 'semantic_similarity']:
                            if metric_name in item and item[metric_name] is not None:
                                if metric_name not in metrics_sums:
                                    metrics_sums[metric_name] = 0
                                    metrics_counts[metric_name] = 0
                                metrics_sums[metric_name] += float(item[metric_name])
                                metrics_counts[metric_name] += 1

                    for metric_name in metrics_sums:
                        if metrics_counts[metric_name] > 0:
                            scores_dict[metric_name] = metrics_sums[metric_name] / metrics_counts[metric_name]

            # In káº¿t quáº£
            if scores_dict:
                for metric_name, avg_score in scores_dict.items():
                    print(f"{metric_name:20}: {avg_score:.3f}")
            else:
                print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y scores trong káº¿t quáº£ evaluation")
                print(f"ğŸ” Type: {type(result)}")
                print(f"ğŸ” Attributes: {[attr for attr in dir(result) if not attr.startswith('_')]}")

        except Exception as e:
            print(f"âŒ Lá»—i khi in summary: {e}")
            print(f"ğŸ” Type cá»§a result: {type(result)}")

        print("="*60)

async def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y evaluation
    """
    # Khá»Ÿi táº¡o evaluator
    evaluator = RAGEvaluator()

    # ÄÆ°á»ng dáº«n file Excel chá»©a test data
    excel_path = "../app/data_test.xlsx"  # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n náº¿u cáº§n

    # Cháº¡y evaluation
    try:
        results = await evaluator.run_evaluation(
            excel_path=excel_path,
            output_path="rag_evaluation_results.json"
        )
        print("ğŸ‰ ÄÃ¡nh giÃ¡ hoÃ n thÃ nh thÃ nh cÃ´ng!")

    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

if __name__ == "__main__":
    # Cháº¡y evaluation
    asyncio.run(main())




