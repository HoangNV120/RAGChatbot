import pandas as pd
import asyncio
import time
import tiktoken
import numpy as np
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity
from langchain_openai import OpenAIEmbeddings
import logging
from typing import Dict, List, Any, Optional

# Import c√°c components c·∫ßn thi·∫øt
from app.master_chatbot import MasterChatbot
from app.vector_store import VectorStore
from app.vector_store_small import VectorStoreSmall
from app.config import settings

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

class DetailedEvaluation:
    def __init__(self):
        """Kh·ªüi t·∫°o evaluation system"""
        print("üöÄ Initializing Detailed Evaluation System...")

        # Kh·ªüi t·∫°o vector stores
        self.vector_store = VectorStore()
        self.vector_store_small = VectorStoreSmall()

        # Kh·ªüi t·∫°o master chatbot
        self.master_chatbot = MasterChatbot(
            vector_store=self.vector_store,
            vector_store_small=self.vector_store_small
        )

        # Kh·ªüi t·∫°o embedding model ƒë·ªÉ t√≠nh similarity
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_model,
            api_key=settings.openai_api_key,
        )

        # Kh·ªüi t·∫°o tiktoken encoder ƒë·ªÉ t√≠nh cost
        self.encoder = tiktoken.encoding_for_model("gpt-4o-mini")

        # Pricing (USD per 1K tokens) - c·∫≠p nh·∫≠t theo gi√° m·ªõi nh·∫•t
        self.pricing = {
            "embedding_input": 0.00002,  # text-embedding-3-large
            "llm_input": 0.00015,        # gpt-4o-mini input
            "llm_output": 0.0006         # gpt-4o-mini output
        }

        print("‚úÖ Evaluation system initialized successfully!")

    def count_tokens(self, text: str) -> int:
        """ƒê·∫øm s·ªë tokens trong text"""
        if not text:
            return 0
        return len(self.encoder.encode(str(text)))

    def calculate_embedding_cost(self, text: str) -> float:
        """T√≠nh cost cho embedding"""
        tokens = self.count_tokens(text)
        return (tokens / 1000) * self.pricing["embedding_input"]

    def calculate_llm_cost(self, input_text: str, output_text: str) -> tuple:
        """T√≠nh cost cho LLM (input v√† output ri√™ng bi·ªát)"""
        input_tokens = self.count_tokens(input_text)
        output_tokens = self.count_tokens(output_text)

        input_cost = (input_tokens / 1000) * self.pricing["llm_input"]
        output_cost = (output_tokens / 1000) * self.pricing["llm_output"]

        return input_cost, output_cost, input_tokens, output_tokens

    async def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """T√≠nh cosine similarity gi·ªØa 2 text s·ª≠ d·ª•ng embedding"""
        try:
            if not text1 or not text2:
                return 0.0

            # T·∫°o embeddings cho c·∫£ 2 text
            embeddings = await self.embeddings.aembed_documents([text1, text2])

            # T√≠nh cosine similarity
            embedding1 = np.array(embeddings[0]).reshape(1, -1)
            embedding2 = np.array(embeddings[1]).reshape(1, -1)

            similarity = cosine_similarity(embedding1, embedding2)[0][0]
            return float(similarity)

        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {e}")
            return 0.0

    async def evaluate_single_query(self, question: str, expected_answer: str, query_index: int) -> Dict[str, Any]:
        """ƒê√°nh gi√° m·ªôt query duy nh·∫•t theo ƒë√∫ng lu·ªìng: Master chatbot ‚Üí RAG_CHAT (n·∫øu c·∫ßn)"""
        print(f"\n{'='*60}")
        print(f"üîç EVALUATING QUERY {query_index + 1}")
        print(f"Question: {question[:100]}...")
        print(f"{'='*60}")

        # Kh·ªüi t·∫°o metrics
        metrics = {
            "question": question,
            "expected_answer": expected_answer,
            "query_index": query_index + 1,

            # Master chatbot phase (ragsmall)
            "master_chatbot_time": 0,
            "ragsmall_search_cost": 0,

            # RAG_CHAT phases (ch·ªâ c√≥ n·∫øu threshold < 0.8)
            "pre_retrieval_time": 0,
            "pre_retrieval_cost": 0,
            "pre_retrieval_tokens": 0,
            "embedding_time": 0,
            "embedding_cost": 0,  # ‚úÖ Th√™m embedding_cost v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
            "vector_search_time": 0,
            "generation_time": 0,
            "generation_cost": 0,

            # Result metrics
            "actual_answer": "",
            "route_used": "",
            "chunks_retrieved": [],
            "ragsmall_similarity": 0,
            "cosine_similarity": 0,
            "accuracy_score": 0,
            "total_time": 0,
            "total_cost": 0,

            # Error tracking
            "errors": []
        }

        total_start_time = time.time()

        try:
            # === PHASE 1: MASTER CHATBOT (Full pipeline) ===
            print("üè† Phase 1: Master Chatbot (full pipeline)...")
            master_start_time = time.time()

            # Ch·∫°y master chatbot - n√≥ s·∫Ω t·ª± ƒë·ªông handle ragsmall ‚Üí RAG_CHAT fallback
            response = await self.master_chatbot.generate_response(question)

            master_end_time = time.time()
            metrics["master_chatbot_time"] = master_end_time - master_start_time
            metrics["actual_answer"] = response.get("output", "")
            metrics["route_used"] = response.get("route_used", "UNKNOWN")

            print(f"   ‚è±Ô∏è  Master chatbot time: {metrics['master_chatbot_time']:.3f}s")
            print(f"   üéØ Route used: {metrics['route_used']}")

            # Thu th·∫≠p ragsmall info n·∫øu c√≥
            ragsmall_info = response.get("ragsmall_info", {})
            if ragsmall_info:
                metrics["ragsmall_similarity"] = ragsmall_info.get("similarity_score", 0)
                print(f"   üìà Ragsmall similarity: {metrics['ragsmall_similarity']:.4f}")

            # T√≠nh ragsmall search cost (embedding cost lu√¥n c√≥)
            ragsmall_embedding_cost = self.calculate_embedding_cost(question)
            metrics["ragsmall_search_cost"] = ragsmall_embedding_cost
            print(f"   üí∞ Ragsmall search cost: ${ragsmall_embedding_cost:.6f}")

            # === THU TH·∫¨P METRICS T·ª™ RAG_CHAT (n·∫øu c√≥ fallback) ===
            if "FALLBACK" in metrics["route_used"] or "RAG_CHAT" in metrics["route_used"]:
                print("üîÑ Phase 2: Extracting RAG_CHAT metrics...")

                # L·∫•y pre-retrieval metrics t·ª´ response
                pre_retrieval_metrics = response.get("pre_retrieval_metrics", {})
                if pre_retrieval_metrics:
                    metrics["pre_retrieval_time"] = pre_retrieval_metrics.get("pre_retrieval_time", 0)
                    metrics["pre_retrieval_cost"] = pre_retrieval_metrics.get("pre_retrieval_cost", 0)
                    metrics["pre_retrieval_tokens"] = pre_retrieval_metrics.get("total_tokens", 0)

                    print(f"   üß† Pre-retrieval time: {metrics['pre_retrieval_time']:.3f}s")
                    print(f"   üí∞ Pre-retrieval cost: ${metrics['pre_retrieval_cost']:.6f}")
                    print(f"   üìä Pre-retrieval tokens: {metrics['pre_retrieval_tokens']}")

                # L·∫•y vector metrics t·ª´ response (thay v√¨ estimate)
                vector_metrics = response.get("vector_metrics", {})
                if vector_metrics:
                    metrics["embedding_time"] = vector_metrics.get("rag_embedding_time", 0)
                    metrics["vector_search_time"] = vector_metrics.get("rag_vector_search_time", 0)

                    print(f"   üîç RAG embedding time: {metrics['embedding_time']:.3f}s")
                    print(f"   üîç RAG vector search time: {metrics['vector_search_time']:.3f}s")

                    # T√≠nh embedding cost t·ª´ subqueries
                    subqueries = response.get("subqueries", [question])
                    total_embedding_cost = 0
                    for subquery in subqueries:
                        total_embedding_cost += self.calculate_embedding_cost(subquery)

                    metrics["embedding_cost"] = total_embedding_cost
                    print(f"   üí∞ RAG embedding cost: ${total_embedding_cost:.6f} (for {len(subqueries)} subqueries)")

                    # L·∫•y generation metrics th·ª±c t·∫ø t·ª´ vector_metrics (ƒë√£ ƒë∆∞·ª£c combine)
                    metrics["generation_time"] = vector_metrics.get("generation_time", 0)

                    # ‚úÖ ∆Øu ti√™n s·ª≠ d·ª•ng generation_cost t·ª´ vector_metrics n·∫øu c√≥
                    if vector_metrics.get("generation_cost", 0) > 0:
                        metrics["generation_cost"] = vector_metrics.get("generation_cost", 0)
                    else:
                        # Fallback: t√≠nh manual n·∫øu vector_metrics kh√¥ng c√≥ generation_cost
                        if metrics["actual_answer"]:
                            subqueries = response.get("subqueries", [question])
                            estimated_context = " ".join([f"Context for: {sq[:100]}" for sq in subqueries])
                            full_prompt = f"System prompt + Context: {estimated_context} + Question: {question}"

                            gen_input_cost, gen_output_cost, _, _ = self.calculate_llm_cost(
                                full_prompt, metrics["actual_answer"]
                            )
                            metrics["generation_cost"] = gen_input_cost + gen_output_cost
                        else:
                            metrics["generation_cost"] = 0

                    generation_tokens = vector_metrics.get("total_tokens", 0)

                    print(f"   ü§ñ Generation time: {metrics['generation_time']:.3f}s")
                    print(f"   üí∞ Generation cost: ${metrics['generation_cost']:.6f}")
                    print(f"   üìä Generation tokens: {generation_tokens}")

                    # L·∫•y chunks th·ª±c t·∫ø t·ª´ retrieved_docs thay v√¨ estimate
                    retrieved_docs = response.get("retrieved_docs", [])
                    chunks_info = []
                    for doc_info in retrieved_docs:
                        chunks_info.append({
                            "content": doc_info.get("content", ""),
                            "content_length": doc_info.get("content_length", 0),
                            "chunk_index": doc_info.get("chunk_index", 0),
                            "metadata": doc_info.get("metadata", {})
                        })

                    metrics["chunks_retrieved"] = chunks_info
                    print(f"   üìÑ Actual chunks retrieved: {len(chunks_info)}")

                    # Log chi ti·∫øt t·ª´ng chunk
                    for i, chunk in enumerate(chunks_info[:3]):  # Ch·ªâ log 3 chunks ƒë·∫ßu
                        print(f"      Chunk {i+1}: {chunk['content'][:100]}... (length: {chunk['content_length']})")
                else:
                    # Fallback estimate n·∫øu kh√¥ng c√≥ vector_metrics
                    remaining_time = metrics["master_chatbot_time"] - metrics.get("pre_retrieval_time", 0)
                    metrics["embedding_time"] = remaining_time * 0.3  # ~30% for embedding
                    metrics["vector_search_time"] = remaining_time * 0.4  # ~40% for search
                    metrics["generation_time"] = remaining_time * 0.3  # ~30% for generation

                    # Estimate embedding cost t·ª´ subqueries
                    subqueries = response.get("subqueries", [question])
                    total_embedding_cost = 0
                    for subquery in subqueries:
                        total_embedding_cost += self.calculate_embedding_cost(subquery)
                    metrics["embedding_cost"] = total_embedding_cost

                    # Estimate generation cost
                    if metrics["actual_answer"]:
                        estimated_context = " ".join([f"Context for: {sq[:100]}" for sq in subqueries])
                        full_prompt = f"System prompt + Context: {estimated_context} + Question: {question}"

                        gen_input_cost, gen_output_cost, _, _ = self.calculate_llm_cost(
                            full_prompt, metrics["actual_answer"]
                        )
                        metrics["generation_cost"] = gen_input_cost + gen_output_cost

                    print(f"   ‚ö†Ô∏è  Using fallback estimates (no vector_metrics)")
                    print(f"   üîç Estimated embedding time: {metrics['embedding_time']:.3f}s")
                    print(f"   üîç Estimated vector search time: {metrics['vector_search_time']:.3f}s")
                    print(f"   ü§ñ Estimated generation time: {metrics['generation_time']:.3f}s")
                    print(f"   üí∞ RAG embedding cost: ${total_embedding_cost:.6f} (estimated)")
                    print(f"   üí∞ Estimated generation cost: ${metrics['generation_cost']:.6f}")

            else:
                # RAGSMALL_MATCH - kh√¥ng v√†o RAG_CHAT
                print("‚úÖ Direct RAGSMALL match - No RAG_CHAT pipeline used")
                # C√°c metrics RAG_CHAT gi·ªØ nguy√™n gi√° tr·ªã 0

            # === PHASE 3: SIMILARITY CALCULATION ===
            print("üìä Phase 3: Answer Quality Assessment...")

            if metrics["actual_answer"] and expected_answer:
                cosine_sim = await self.calculate_cosine_similarity(metrics["actual_answer"], expected_answer)
                metrics["cosine_similarity"] = cosine_sim
                metrics["accuracy_score"] = 1 if cosine_sim >= 0.7 else 0

                print(f"   üìà Cosine similarity: {cosine_sim:.4f}")
                print(f"   ‚úÖ Accuracy score: {metrics['accuracy_score']}")
            else:
                metrics["cosine_similarity"] = 0
                metrics["accuracy_score"] = 0
                metrics["errors"].append("Missing answer for similarity calculation")

        except Exception as e:
            logger.error(f"Error evaluating query {query_index + 1}: {e}")
            metrics["errors"].append(f"Evaluation error: {str(e)}")
            metrics["actual_answer"] = "ERROR"

        finally:
            # === FINAL METRICS ===
            total_end_time = time.time()
            metrics["total_time"] = total_end_time - total_start_time

            # T√≠nh total cost (tr√°nh tr√πng l·∫∑p embedding cost)
            if "FALLBACK" in metrics["route_used"] or "RAG_CHAT" in metrics["route_used"]:
                # RAG_CHAT flow: ragsmall + pre_retrieval + embedding + generation
                metrics["total_cost"] = (
                    metrics["ragsmall_search_cost"] +
                    metrics["pre_retrieval_cost"] +
                    metrics["embedding_cost"] +  # ƒê√£ t√≠nh t·ª´ subqueries
                    metrics["generation_cost"]
                )
            else:
                # RAGSMALL_MATCH flow: ch·ªâ c√≥ ragsmall cost
                metrics["total_cost"] = metrics["ragsmall_search_cost"]

            print(f"\nüìã SUMMARY for Query {query_index + 1}:")
            print(f"   ‚è±Ô∏è  Total time: {metrics['total_time']:.3f}s")
            print(f"   üí∞ Total cost: ${metrics['total_cost']:.6f}")
            print(f"   üéØ Route used: {metrics['route_used']}")
            print(f"   ‚úÖ Accuracy: {metrics['accuracy_score']}")

            if metrics["errors"]:
                print(f"   ‚ö†Ô∏è  Errors: {len(metrics['errors'])}")

        return metrics

    async def run_evaluation(self, testcase_file: str = "testcase.xlsx"):
        """Ch·∫°y evaluation cho to√†n b·ªô testcase"""
        print("üöÄ Starting Detailed RAG Evaluation...")
        print(f"üìÅ Reading testcase from: {testcase_file}")

        # ƒê·ªçc testcase
        try:
            df = pd.read_excel(testcase_file)
            print(f"üìä Loaded {len(df)} test cases")

            # Gi·∫£ ƒë·ªãnh c√≥ 2 c·ªôt: question v√† answer
            if 'question' not in df.columns or 'answer' not in df.columns:
                # N·∫øu kh√¥ng c√≥ t√™n c·ªôt chu·∫©n, l·∫•y 2 c·ªôt ƒë·∫ßu ti√™n
                df.columns = ['question', 'answer'] + list(df.columns[2:])
                print("‚ö†Ô∏è  Using first 2 columns as question and answer")

        except Exception as e:
            logger.error(f"Error reading testcase file: {e}")
            return

        # Ch·∫°y evaluation cho t·ª´ng query
        all_results = []
        total_queries = len(df)

        print(f"\nüîÑ Processing {total_queries} queries...")

        for index, row in df.iterrows():
            question = str(row['question'])
            expected_answer = str(row['answer'])

            # Skip empty rows
            if not question or question.lower() in ['nan', 'none', '']:
                continue

            print(f"\n{'üîÑ' * 20} QUERY {index + 1}/{total_queries} {'üîÑ' * 20}")

            result = await self.evaluate_single_query(question, expected_answer, index)
            all_results.append(result)

            # Delay nh·ªè ƒë·ªÉ tr√°nh rate limiting
            await asyncio.sleep(0.5)

        # === ANALYSIS & REPORTING ===
        print(f"\n{'üéØ' * 20} ANALYSIS PHASE {'üéØ' * 20}")
        await self.generate_report(all_results)

    async def generate_report(self, results: List[Dict[str, Any]]):
        """T·∫°o b√°o c√°o chi ti·∫øt v√† summary"""
        print("üìä Generating detailed report...")

        # T·∫°o detailed DataFrame
        detailed_data = []
        for result in results:
            row = {
                "Query_Index": result["query_index"],
                "Question": result["question"],
                "Expected_Answer": result["expected_answer"],
                "Actual_Answer": result["actual_answer"],
                "Route_Used": result["route_used"],

                # Master chatbot phase
                "Master_Chatbot_Time_s": round(result["master_chatbot_time"], 4),
                "Ragsmall_Search_Cost_USD": round(result["ragsmall_search_cost"], 8),
                "Ragsmall_Similarity": round(result["ragsmall_similarity"], 4),

                # RAG_CHAT phases (0 if not used)
                "Pre_Retrieval_Time_s": round(result["pre_retrieval_time"], 4),
                "Embedding_Time_s": round(result["embedding_time"], 4),
                "Vector_Search_Time_s": round(result["vector_search_time"], 4),
                "Generation_Time_s": round(result["generation_time"], 4),

                # RAG_CHAT costs (0 if not used)
                "Pre_Retrieval_Cost_USD": round(result["pre_retrieval_cost"], 8),
                "Embedding_Cost_USD": round(result["embedding_cost"], 8),
                "Generation_Cost_USD": round(result["generation_cost"], 8),

                # Totals
                "Total_Time_s": round(result["total_time"], 4),
                "Total_Cost_USD": round(result["total_cost"], 8),

                # Quality metrics
                "Cosine_Similarity": round(result["cosine_similarity"], 4),
                "Accuracy_Score": result["accuracy_score"],
                "Chunks_Count": len(result["chunks_retrieved"]),
                "Has_Errors": len(result["errors"]) > 0,
                "Errors": "; ".join(result["errors"]) if result["errors"] else ""
            }
            detailed_data.append(row)

        detailed_df = pd.DataFrame(detailed_data)

        # T√≠nh summary statistics
        summary_stats = {
            "Total_Queries": len(results),
            "Successful_Queries": len([r for r in results if not r["errors"]]),
            "Failed_Queries": len([r for r in results if r["errors"]]),

            # Route distribution
            "RAGSMALL_MATCH_Count": len([r for r in results if r["route_used"] == "RAGSMALL_MATCH"]),
            "RAG_CHAT_FALLBACK_Count": len([r for r in results if "FALLBACK" in r["route_used"]]),

            # Timing averages
            "Avg_Master_Chatbot_Time_s": np.mean([r["master_chatbot_time"] for r in results]),
            "Avg_Pre_Retrieval_Time_s": np.mean([r["pre_retrieval_time"] for r in results if r["pre_retrieval_time"] > 0]),
            "Avg_Embedding_Time_s": np.mean([r["embedding_time"] for r in results if r["embedding_time"] > 0]),
            "Avg_Vector_Search_Time_s": np.mean([r["vector_search_time"] for r in results if r["vector_search_time"] > 0]),
            "Avg_Generation_Time_s": np.mean([r["generation_time"] for r in results if r["generation_time"] > 0]),
            "Avg_Total_Time_s": np.mean([r["total_time"] for r in results]),

            # ‚úÖ Th√™m RAG Chat Latency v√† Ragsmall Latency
            "Avg_RAG_Chat_Latency_s": np.mean([
                r["pre_retrieval_time"] + r["embedding_time"] + r["vector_search_time"] + r["generation_time"]
                for r in results if "FALLBACK" in r["route_used"] or "RAG_CHAT" in r["route_used"]
            ]) if [r for r in results if "FALLBACK" in r["route_used"] or "RAG_CHAT" in r["route_used"]] else 0.0,

            "Avg_Ragsmall_Latency_s": np.mean([
                r["master_chatbot_time"] for r in results if r["route_used"] == "RAGSMALL_MATCH"
            ]) if [r for r in results if r["route_used"] == "RAGSMALL_MATCH"] else 0.0,

            # Cost averages
            "Avg_Ragsmall_Search_Cost_USD": np.mean([r["ragsmall_search_cost"] for r in results]),
            "Avg_Pre_Retrieval_Cost_USD": np.mean([r["pre_retrieval_cost"] for r in results if r["pre_retrieval_cost"] > 0]),
            "Avg_Embedding_Cost_USD": np.mean([r["embedding_cost"] for r in results if r["embedding_cost"] > 0]),
            "Avg_Generation_Cost_USD": np.mean([r["generation_cost"] for r in results if r["generation_cost"] > 0]),
            "Avg_Total_Cost_USD": np.mean([r["total_cost"] for r in results]),

            # ‚úÖ Th√™m RAG Chat Cost v√† Ragsmall Cost
            "Avg_RAG_Chat_Cost_USD": np.mean([
                r["total_cost"]  # ‚úÖ L·∫•y total_cost (ƒë√£ bao g·ªìm t·∫•t c·∫£) cho RAG_CHAT route
                for r in results if "FALLBACK" in r["route_used"] or "RAG_CHAT" in r["route_used"]
            ]) if [r for r in results if "FALLBACK" in r["route_used"] or "RAG_CHAT" in r["route_used"]] else 0.0,

            "Avg_Ragsmall_Cost_USD": np.mean([
                r["ragsmall_search_cost"]  # ‚úÖ Ch·ªâ l·∫•y ragsmall cost cho RAGSMALL_MATCH route
                for r in results if r["route_used"] == "RAGSMALL_MATCH"
            ]) if [r for r in results if r["route_used"] == "RAGSMALL_MATCH"] else 0.0,

            # Totals
            "Total_Cost_USD": np.sum([r["total_cost"] for r in results]),
            "Total_Time_s": np.sum([r["total_time"] for r in results]),

            # Quality metrics
            "Avg_Ragsmall_Similarity": np.mean([r["ragsmall_similarity"] for r in results if r["ragsmall_similarity"] > 0]),
            "Avg_Cosine_Similarity": np.mean([r["cosine_similarity"] for r in results]),
            "Accuracy_Rate": np.mean([r["accuracy_score"] for r in results]),
            "High_Accuracy_Count": len([r for r in results if r["accuracy_score"] == 1]),
        }

        # Handle NaN values for cases where no RAG_CHAT was used
        for key in summary_stats:
            if np.isnan(summary_stats[key]):
                summary_stats[key] = 0.0

        # T·∫°o summary DataFrame
        summary_df = pd.DataFrame([summary_stats])

        # === SAVE TO EXCEL ===
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rag_evaluation_detailed_{timestamp}.xlsx"

        print(f"üíæ Saving report to: {filename}")

        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Sheet 1: Detailed results
            detailed_df.to_excel(writer, sheet_name='Detailed_Results', index=False)

            # Sheet 2: Summary statistics
            summary_df.to_excel(writer, sheet_name='Summary_Statistics', index=False)

            # Sheet 3: Chunks information (t·ª´ retrieved_docs th·ª±c t·∫ø)
            chunks_data = []
            for i, result in enumerate(results):  # T·∫•t c·∫£ queries, kh√¥ng ch·ªâ 10 ƒë·∫ßu
                for chunk in result["chunks_retrieved"]:
                    chunks_data.append({
                        "Query_Index": result["query_index"],
                        "Chunk_Index": chunk.get("chunk_index", 0),
                        "Content": chunk.get("content", ""),
                        "Content_Length": chunk.get("content_length", 0),
                        "Metadata": str(chunk.get("metadata", {}))
                    })

            if chunks_data:
                chunks_df = pd.DataFrame(chunks_data)
                chunks_df.to_excel(writer, sheet_name='Retrieved_Chunks', index=False)
            else:
                # T·∫°o empty sheet n·∫øu kh√¥ng c√≥ chunks
                empty_chunks_df = pd.DataFrame({
                    "Query_Index": [],
                    "Chunk_Index": [],
                    "Content": [],
                    "Content_Length": [],
                    "Metadata": []
                })
                empty_chunks_df.to_excel(writer, sheet_name='Retrieved_Chunks', index=False)

        # === PRINT SUMMARY ===
        print(f"\n{'üéØ' * 20} EVALUATION SUMMARY {'üéØ' * 20}")
        print(f"üìä Total Queries: {summary_stats['Total_Queries']}")
        print(f"‚úÖ Successful: {summary_stats['Successful_Queries']}")
        print(f"‚ùå Failed: {summary_stats['Failed_Queries']}")
        print(f"üè† RAGSMALL matches: {summary_stats['RAGSMALL_MATCH_Count']}")
        print(f"üîÑ RAG_CHAT fallbacks: {summary_stats['RAG_CHAT_FALLBACK_Count']}")
        print(f"üéØ Accuracy Rate: {summary_stats['Accuracy_Rate']:.2%}")
        print(f"üìà Avg Cosine Similarity: {summary_stats['Avg_Cosine_Similarity']:.4f}")
        print(f"‚è±Ô∏è  Avg Total Time: {summary_stats['Avg_Total_Time_s']:.3f}s")
        print(f"üí∞ Avg Total Cost: ${summary_stats['Avg_Total_Cost_USD']:.6f}")
        print(f"üí∞ Total Cost: ${summary_stats['Total_Cost_USD']:.6f}")
        print(f"üìÅ Report saved: {filename}")
        print(f"{'üéØ' * 60}")

async def main():
    """Main function to run the evaluation"""
    evaluation = DetailedEvaluation()
    await evaluation.run_evaluation()

if __name__ == "__main__":
    asyncio.run(main())
