import pandas as pd
import requests
import json
import time
from datetime import datetime
from difflib import SequenceMatcher
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class ChatbotAccuracyTester:
    def __init__(self, api_url="http://localhost:8000/chat"):
        self.api_url = api_url
        self.results = []
        
    def preprocess_text(self, text):
        """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ƒë·ªÉ so s√°nh"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower().strip()
        # Lo·∫°i b·ªè d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = re.sub(r'[^\w\s]', ' ', text)
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text)
        return text
    
    def calculate_similarity_scores(self, response, expected):
        """T√≠nh nhi·ªÅu lo·∫°i ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng"""
        
        # Preprocess texts
        response_clean = self.preprocess_text(response)
        expected_clean = self.preprocess_text(expected)
        
        if not response_clean or not expected_clean:
            return {
                'sequence_similarity': 0.0,
                'tfidf_similarity': 0.0,
                'word_overlap': 0.0,
                'length_similarity': 0.0
            }
        
        # 1. Sequence similarity (SequenceMatcher)
        sequence_sim = SequenceMatcher(None, response_clean, expected_clean).ratio()
        
        # 2. TF-IDF Cosine similarity
        try:
            vectorizer = TfidfVectorizer().fit([response_clean, expected_clean])
            vectors = vectorizer.transform([response_clean, expected_clean])
            tfidf_sim = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
        except:
            tfidf_sim = 0.0
        
        # 3. Word overlap percentage
        response_words = set(response_clean.split())
        expected_words = set(expected_clean.split())
        if len(expected_words) > 0:
            word_overlap = len(response_words.intersection(expected_words)) / len(expected_words)
        else:
            word_overlap = 0.0
        
        # 4. Length similarity (ph·∫°t qu√° ng·∫Øn ho·∫∑c qu√° d√†i)
        len_response = len(response_clean)
        len_expected = len(expected_clean)
        if len_expected > 0:
            length_ratio = min(len_response, len_expected) / max(len_response, len_expected)
        else:
            length_ratio = 0.0
        
        return {
            'sequence_similarity': sequence_sim,
            'tfidf_similarity': tfidf_sim,
            'word_overlap': word_overlap,
            'length_similarity': length_ratio
        }
    
    def calculate_composite_score(self, similarities):
        """T√≠nh ƒëi·ªÉm t·ªïng h·ª£p t·ª´ c√°c ƒëi·ªÉm similarity"""
        weights = {
            'sequence_similarity': 0.3,
            'tfidf_similarity': 0.4,
            'word_overlap': 0.2,
            'length_similarity': 0.1
        }
        
        composite_score = sum(similarities[key] * weights[key] for key in weights)
        return composite_score
    
    def classify_result(self, composite_score, similarities):
        """Ph√¢n lo·∫°i k·∫øt qu·∫£ d·ª±a tr√™n ƒëi·ªÉm s·ªë"""
        if composite_score >= 0.8:
            return "EXCELLENT"
        elif composite_score >= 0.6:
            return "GOOD"
        elif composite_score >= 0.4:
            return "FAIR"
        elif composite_score >= 0.2:
            return "POOR"
        else:
            return "VERY_POOR"
    
    def call_api(self, question, timeout=30):
        """G·ªçi API chatbot"""
        try:
            # Ki·ªÉm tra question c√≥ h·ª£p l·ªá kh√¥ng
            if not question or not str(question).strip():
                return "Empty question", False, "Question is empty"
            
            payload = {
                "chatInput": str(question).strip(),
                "sessionId": "accuracy_test_session"
            }
            
            response = requests.post(
                self.api_url,
                json=payload,
                timeout=timeout,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('output', ''), True, None
            else:
                return f"API Error: {response.status_code}", False, f"Status: {response.status_code}"
                
        except requests.exceptions.Timeout:
            return "Timeout", False, "Request timeout"
        except requests.exceptions.ConnectionError:
            return "Connection Error", False, "Cannot connect to API"
        except Exception as e:
            return f"Error: {str(e)}", False, str(e)
    
    def test_accuracy(self, testcase_file="testcase.xlsx"):
        """Ch·∫°y test accuracy v·ªõi file testcase"""
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu test accuracy chatbot")
        print(f"üìä API URL: {self.api_url}")
        print(f"üìÅ Test file: {testcase_file}")
        
        # ƒê·ªçc file testcase
        try:
            df = pd.read_excel(testcase_file)
            print(f"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(df)} test cases")
            print(f"üìã Columns: {list(df.columns)}")
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
            return
        
        # Ki·ªÉm tra columns c·∫ßn thi·∫øt
        required_columns = ['question', 'answer']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"‚ùå Thi·∫øu columns: {missing_columns}")
            return
        
        # Ki·ªÉm tra API c√≥ ho·∫°t ƒë·ªông kh√¥ng
        print("\nüîç Ki·ªÉm tra API...")
        test_response, success, error = self.call_api("Hello test", timeout=10)
        if not success:
            print(f"‚ùå API kh√¥ng ho·∫°t ƒë·ªông: {error}")
            return
        print("‚úÖ API ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng")
        
        # Ch·∫°y test
        print(f"\nüß™ B·∫Øt ƒë·∫ßu test {len(df)} c√¢u h·ªèi...")
        
        for idx, row in df.iterrows():
            question = str(row['question']) if pd.notna(row['question']) else ""
            expected_answer = str(row['answer']) if pd.notna(row['answer']) else ""
            context = str(row.get('context', '')) if pd.notna(row.get('context', '')) else ""
            
            # B·ªè qua test n·∫øu question ho·∫∑c answer r·ªóng
            if not question.strip() or not expected_answer.strip():
                print(f"\n‚ö†Ô∏è  Test {idx + 1}/{len(df)}: D·ªØ li·ªáu r·ªóng - b·ªè qua")
                continue
            
            print(f"\nüìù Test {idx + 1}/{len(df)}: {question[:50]}...")
            
            # G·ªçi API
            api_response, success, error = self.call_api(question)
            
            if success:
                # T√≠nh c√°c ƒëi·ªÉm similarity
                similarities = self.calculate_similarity_scores(api_response, expected_answer)
                composite_score = self.calculate_composite_score(similarities)
                result_class = self.classify_result(composite_score, similarities)
                
                # L∆∞u k·∫øt qu·∫£
                result_data = {
                    'test_index': idx + 1,
                    'question': question,
                    'expected_answer': expected_answer,
                    'api_response': api_response,
                    'context': context,
                    'sequence_similarity': similarities['sequence_similarity'],
                    'tfidf_similarity': similarities['tfidf_similarity'],
                    'word_overlap': similarities['word_overlap'],
                    'length_similarity': similarities['length_similarity'],
                    'composite_score': composite_score,
                    'result_class': result_class,
                    'api_success': True,
                    'api_error': None
                }
                
                print(f"   üìä Score: {composite_score:.3f} ({result_class})")
                print(f"   üîç TF-IDF: {similarities['tfidf_similarity']:.3f}, Word Overlap: {similarities['word_overlap']:.3f}")
                
            else:
                # API l·ªói
                result_data = {
                    'test_index': idx + 1,
                    'question': question,
                    'expected_answer': expected_answer,
                    'api_response': api_response,
                    'context': context,
                    'sequence_similarity': 0.0,
                    'tfidf_similarity': 0.0,
                    'word_overlap': 0.0,
                    'length_similarity': 0.0,
                    'composite_score': 0.0,
                    'result_class': 'API_ERROR',
                    'api_success': False,
                    'api_error': error
                }
                
                print(f"   ‚ùå L·ªói API: {error}")
            
            self.results.append(result_data)
            
            # Ngh·ªâ 1 gi√¢y gi·ªØa c√°c request
            time.sleep(1)
        
        # T·∫°o b√°o c√°o
        self.generate_accuracy_report()
    
    def generate_accuracy_report(self):
        """T·∫°o b√°o c√°o accuracy v√† xu·∫•t file Excel"""
        
        if not self.results:
            print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ t·∫°o b√°o c√°o")
            return
        
        # T·∫°o DataFrame t·ª´ k·∫øt qu·∫£
        df_results = pd.DataFrame(self.results)
        
        # Th·ªëng k√™ t·ªïng qu√°t
        total_tests = len(df_results)
        api_errors = len(df_results[df_results['result_class'] == 'API_ERROR'])
        successful_tests = total_tests - api_errors
        
        print(f"\nüìä B√ÅOC√ÅO ACCURACY:")
        print(f"   üìà T·ªïng s·ªë test: {total_tests}")
        print(f"   üî• API errors: {api_errors}")
        print(f"   ‚úÖ Successful tests: {successful_tests}")
        
        if successful_tests > 0:
            # Ph√¢n lo·∫°i k·∫øt qu·∫£
            excellent = len(df_results[df_results['result_class'] == 'EXCELLENT'])
            good = len(df_results[df_results['result_class'] == 'GOOD'])
            fair = len(df_results[df_results['result_class'] == 'FAIR'])
            poor = len(df_results[df_results['result_class'] == 'POOR'])
            very_poor = len(df_results[df_results['result_class'] == 'VERY_POOR'])
            
            print(f"\nüéØ PH√ÇN LO·∫†I K·∫æT QU·∫¢:")
            print(f"   üåü EXCELLENT (‚â•0.8): {excellent} ({excellent/successful_tests*100:.1f}%)")
            print(f"   ‚úÖ GOOD (‚â•0.6): {good} ({good/successful_tests*100:.1f}%)")
            print(f"   üî∂ FAIR (‚â•0.4): {fair} ({fair/successful_tests*100:.1f}%)")
            print(f"   ‚ö†Ô∏è  POOR (‚â•0.2): {poor} ({poor/successful_tests*100:.1f}%)")
            print(f"   ‚ùå VERY_POOR (<0.2): {very_poor} ({very_poor/successful_tests*100:.1f}%)")
            
            # T√≠nh accuracy t·ªïng h·ª£p
            avg_score = df_results[df_results['result_class'] != 'API_ERROR']['composite_score'].mean()
            
            # Accuracy c√≥ tr·ªçng s·ªë: EXCELLENT=1, GOOD=0.8, FAIR=0.6, POOR=0.4, VERY_POOR=0.2
            weighted_score = (excellent * 1.0 + good * 0.8 + fair * 0.6 + poor * 0.4 + very_poor * 0.2) / successful_tests
            
            print(f"\nüìä ƒêI·ªÇM S·ªê T·ªîNG H·ª¢P:")
            print(f"   üéØ Average Composite Score: {avg_score:.3f}")
            print(f"   üèÜ Weighted Accuracy: {weighted_score:.3f} ({weighted_score*100:.1f}%)")
            
            # Top similarity scores
            print(f"\nüìà ƒêI·ªÇM SIMILARITY TRUNG B√åNH:")
            successful_df = df_results[df_results['result_class'] != 'API_ERROR']
            print(f"   üî§ TF-IDF Similarity: {successful_df['tfidf_similarity'].mean():.3f}")
            print(f"   üìù Sequence Similarity: {successful_df['sequence_similarity'].mean():.3f}")
            print(f"   üîó Word Overlap: {successful_df['word_overlap'].mean():.3f}")
            print(f"   üìè Length Similarity: {successful_df['length_similarity'].mean():.3f}")
        
        # L∆∞u k·∫øt qu·∫£ chi ti·∫øt
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"chatbot_accuracy_test_{timestamp}.xlsx"
        
        # T·∫°o file Excel v·ªõi multiple sheets
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # Sheet 1: K·∫øt qu·∫£ chi ti·∫øt
            df_results.to_excel(writer, sheet_name='Detailed_Results', index=False)
            
            # Sheet 2: Th·ªëng k√™ t√≥m t·∫Øt
            if successful_tests > 0:
                summary_data = {
                    'Metric': [
                        'Total Tests', 'API Errors', 'Successful Tests',
                        'EXCELLENT', 'GOOD', 'FAIR', 'POOR', 'VERY_POOR',
                        'Average Composite Score', 'Weighted Accuracy',
                        'Avg TF-IDF Similarity', 'Avg Sequence Similarity',
                        'Avg Word Overlap', 'Avg Length Similarity'
                    ],
                    'Value': [
                        total_tests, api_errors, successful_tests,
                        excellent, good, fair, poor, very_poor,
                        f"{avg_score:.3f}", f"{weighted_score:.3f}",
                        f"{successful_df['tfidf_similarity'].mean():.3f}",
                        f"{successful_df['sequence_similarity'].mean():.3f}",
                        f"{successful_df['word_overlap'].mean():.3f}",
                        f"{successful_df['length_similarity'].mean():.3f}"
                    ],
                    'Percentage': [
                        '100%', f"{api_errors/total_tests*100:.1f}%", f"{successful_tests/total_tests*100:.1f}%",
                        f"{excellent/successful_tests*100:.1f}%", f"{good/successful_tests*100:.1f}%",
                        f"{fair/successful_tests*100:.1f}%", f"{poor/successful_tests*100:.1f}%",
                        f"{very_poor/successful_tests*100:.1f}%",
                        f"{avg_score*100:.1f}%", f"{weighted_score*100:.1f}%",
                        f"{successful_df['tfidf_similarity'].mean()*100:.1f}%",
                        f"{successful_df['sequence_similarity'].mean()*100:.1f}%",
                        f"{successful_df['word_overlap'].mean()*100:.1f}%",
                        f"{successful_df['length_similarity'].mean()*100:.1f}%"
                    ]
                }
                
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        print(f"\nüíæ K·∫øt qu·∫£ ƒë√£ l∆∞u: {output_file}")
        
        # L∆∞u b√°o c√°o text
        summary_file = f"chatbot_accuracy_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("CHATBOT ACCURACY TEST REPORT\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Test time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"API URL: {self.api_url}\n")
            f.write(f"Total tests: {total_tests}\n")
            f.write(f"API errors: {api_errors}\n")
            f.write(f"Successful tests: {successful_tests}\n\n")
            
            if successful_tests > 0:
                f.write("CLASSIFICATION RESULTS:\n")
                f.write(f"EXCELLENT (‚â•0.8): {excellent} ({excellent/successful_tests*100:.1f}%)\n")
                f.write(f"GOOD (‚â•0.6): {good} ({good/successful_tests*100:.1f}%)\n")
                f.write(f"FAIR (‚â•0.4): {fair} ({fair/successful_tests*100:.1f}%)\n")
                f.write(f"POOR (‚â•0.2): {poor} ({poor/successful_tests*100:.1f}%)\n")
                f.write(f"VERY_POOR (<0.2): {very_poor} ({very_poor/successful_tests*100:.1f}%)\n\n")
                f.write(f"OVERALL SCORES:\n")
                f.write(f"Average Composite Score: {avg_score:.3f}\n")
                f.write(f"Weighted Accuracy: {weighted_score:.3f} ({weighted_score*100:.1f}%)\n")
        
        print(f"üìã B√°o c√°o t√≥m t·∫Øt: {summary_file}")

if __name__ == "__main__":
    # T·∫°o tester
    tester = ChatbotAccuracyTester()
    
    # Ch·∫°y test
    tester.test_accuracy("testcase.xlsx")
