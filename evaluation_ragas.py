import pandas as pd
import numpy as np
import asyncio
import time
import json
from typing import List, Dict, Any
from datetime import datetime
import logging

# RAGAS imports - updated for latest version
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_similarity
    )
    from datasets import Dataset
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    print("‚úÖ RAGAS imports successful")
except ImportError as e:
    print(f"RAGAS import error: {e}")
    print("Please install latest RAGAS: pip install ragas --upgrade")
    print("Also install: pip install datasets")

# Additional required imports for Excel export
try:
    import openpyxl
    print("‚úÖ openpyxl available for Excel export")
except ImportError:
    print("‚ö†Ô∏è Installing openpyxl for Excel export...")
    import subprocess
    subprocess.run(["pip", "install", "openpyxl"])
    import openpyxl

# Internal imports
from app.master_chatbot import MasterChatbot
from app.document_processor import DocumentProcessor
from app.vector_store_small import VectorStoreSmall
from app.MultiModelChatAPI import MultiModelChatAPI
from app.config import settings
from ragas_config import RAGAS_METRICS, EVAL_SETTINGS, QUALITY_THRESHOLDS

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGASEvaluator:
    def __init__(self):
        """
        Kh·ªüi t·∫°o RAGAS Evaluator v·ªõi MultiModelAPI
        """
        # Initialize MasterChatbot components
        self.master_chatbot = None
        self.doc_processor = None
        self.vector_store_small = None

        # Initialize MultiModelAPI for RAGAS evaluation
        self.multi_model_api = MultiModelChatAPI(
            api_key=settings.multi_model_api_key,
            model_name="gpt-4o-mini",  # S·ª≠ d·ª•ng model t·ªët cho evaluation
            api_url=settings.multi_model_api_url,
        )

        # Wrap MultiModelAPI cho RAGAS
        self.llm_wrapper = LangchainLLMWrapper(self.multi_model_api)

        # Results storage
        self.results = []
        self.latency_results = []

    async def initialize_chatbot(self):
        """
        Kh·ªüi t·∫°o MasterChatbot v·ªõi dual vector stores
        """
        try:
            print("üöÄ Initializing RAG Chatbot components...")

            # Initialize Document processor
            import os
            data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "app", "data")
            self.doc_processor = DocumentProcessor(data_dir=data_dir)

            # Initialize VectorStoreSmall
            self.vector_store_small = VectorStoreSmall()

            # Initialize Master Chatbot
            self.master_chatbot = MasterChatbot(
                vector_store=self.doc_processor.vector_store,
                vector_store_small=self.vector_store_small
            )

            print("‚úÖ MasterChatbot initialized successfully!")

        except Exception as e:
            logger.error(f"Error initializing chatbot: {e}")
            raise

    def load_testcase_data(self, file_path: str) -> pd.DataFrame:
        """
        ƒê·ªçc d·ªØ li·ªáu t·ª´ file Excel testcase
        """
        try:
            df = pd.read_excel(file_path)
            print(f"üìä Loaded {len(df)} test cases from {file_path}")
            print(f"Columns: {df.columns.tolist()}")

            # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
            required_columns = ['question', 'answer']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                # Th·ª≠ c√°c t√™n c·ªôt kh√°c
                column_mapping = {
                    'Question': 'question',
                    'Q': 'question',
                    'query': 'question',
                    'Answer': 'answer',
                    'A': 'answer',
                    'response': 'answer',
                    'expected_answer': 'answer'
                }

                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns and new_col in missing_columns:
                        df = df.rename(columns={old_col: new_col})
                        missing_columns.remove(new_col)
                        print(f"‚úÖ Mapped column '{old_col}' to '{new_col}'")

            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")

            # L·ªçc b·ªè c√°c d√≤ng tr·ªëng
            df = df.dropna(subset=['question', 'answer'])
            print(f"üìù After filtering: {len(df)} valid test cases")

            return df

        except Exception as e:
            logger.error(f"Error loading testcase data: {e}")
            raise

    async def generate_rag_response(self, question: str, session_id: str = None) -> Dict:
        """
        Sinh response t·ª´ RAG system v√† ƒëo latency, ƒë·ªìng th·ªùi l·∫•y contexts th·ª±c t·ª´ retrieval
        """
        start_time = time.time()

        try:
            # Generate response using MasterChatbot
            response = await self.master_chatbot.generate_response(
                query=question,
                session_id=session_id or f"eval_{int(time.time())}"
            )

            end_time = time.time()
            latency = end_time - start_time

            # L·∫•y contexts th·ª±c t·ª´ retrieval process
            contexts = await self.extract_real_contexts_from_rag(question, response)

            return {
                'answer': response.get('output', ''),
                'latency': latency,
                'route_used': response.get('route_used', 'UNKNOWN'),
                'session_id': response.get('session_id', ''),
                'contexts': contexts
            }

        except Exception as e:
            logger.error(f"Error generating RAG response: {e}")
            end_time = time.time()
            latency = end_time - start_time

            return {
                'answer': f"Error: {str(e)}",
                'latency': latency,
                'route_used': 'ERROR',
                'session_id': '',
                'contexts': [f"Error retrieving context: {str(e)}"]
            }

    async def extract_real_contexts_from_rag(self, question: str, response: Dict) -> List[str]:
        """
        L·∫•y contexts th·ª±c t·ª´ RAG retrieval process - s·ª≠a ƒë·ªÉ l·∫•y ƒë√∫ng t·ª´ vector store t∆∞∆°ng ·ª©ng
        """
        contexts = []

        try:
            route_used = response.get('route_used', '')

            if route_used == 'RAGSMALL_MATCH':
                # N·∫øu d√πng ragsmall, l·∫•y context t·ª´ vector_store_small nh∆∞ trong master_chatbot
                print(f"   üìñ Getting contexts from vector_store_small for RAGSMALL route")

                ragsmall_info = response.get('ragsmall_info', {})

                # L·∫•y contexts t·ª´ vector_store_small search (gi·ªëng nh∆∞ master_chatbot)
                try:
                    ragsmall_results = await self.vector_store_small.similarity_search_with_score(
                        query=question,
                        k=3  # L·∫•y top 3 ƒë·ªÉ c√≥ ƒë·ªß context
                    )

                    for doc, score in ragsmall_results:

                        # Th√™m answer t·ª´ metadata l√†m context
                        if 'answer' in doc.metadata:
                            contexts.append(doc.metadata['answer'])

                        # Limit contexts
                        if len(contexts) >= 4:
                            break

                except Exception as e:
                    logger.warning(f"Error getting contexts from vector_store_small: {e}")
                    # Fallback: s·ª≠ d·ª•ng info t·ª´ response
                    if 'matched_question' in ragsmall_info:
                        contexts.append(ragsmall_info['matched_question'])
                    if 'matched_category' in ragsmall_info:
                        contexts.append(f"Category: {ragsmall_info['matched_category']}")

            elif 'RAG_CHAT' in route_used:
                # N·∫øu d√πng RAG_CHAT, l·∫•y contexts t·ª´ vector_store ch√≠nh
                print(f"   üìñ Getting contexts from main vector_store for RAG_CHAT route")
                contexts = await self.get_rag_chat_contexts(question)

            else:
                # Fallback: th·ª±c hi·ªán retrieval ƒë·ªÉ l·∫•y contexts t·ª´ main vector store
                print(f"   üìñ Getting contexts from main vector_store for {route_used} route")
                contexts = await self.get_rag_chat_contexts(question)

        except Exception as e:
            logger.warning(f"Error extracting real contexts: {e}")
            # Fallback: l·∫•y contexts b·∫±ng c√°ch search tr·ª±c ti·∫øp t·ª´ main vector store
            contexts = await self.get_rag_chat_contexts(question)

        # ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 1 context
        if not contexts:
            contexts = [f"No relevant context found for: {question[:100]}..."]

        print(f"   üìñ Total contexts extracted: {len(contexts)}")
        return contexts

    async def get_rag_chat_contexts(self, question: str) -> List[str]:
        """
        L·∫•y contexts th·ª±c t·ª´ RAG chat retrieval process
        """
        contexts = []

        try:
            # S·ª≠ d·ª•ng query rewriter nh∆∞ trong rag_chat
            rewrite_result = await self.master_chatbot.rag_chat.query_rewriter.analyze_and_rewrite(question)
            processed_query = rewrite_result.get("rewritten_query", question)
            subqueries = rewrite_result.get("subqueries", [processed_query])

            # Th·ª±c hi·ªán retrieval nh∆∞ trong rag_chat
            k_per_query = max(1, min(3, 6 // len(subqueries)))

            # S·ª≠ d·ª•ng batch search nh∆∞ trong rag_chat
            all_results = await self.master_chatbot.rag_chat.vector_store.batch_similarity_search(
                subqueries,
                k=k_per_query
            )

            # K·∫øt h·ª£p v√† lo·∫°i b·ªè tr√πng l·∫∑p nh∆∞ trong rag_chat
            seen_content = set()
            for results in all_results:
                if isinstance(results, list):
                    for doc in results:
                        content_hash = hash(doc.page_content[:200])
                        if content_hash not in seen_content:
                            seen_content.add(content_hash)
                            contexts.append(doc.page_content)

                            # Limit to 3-4 contexts ƒë·ªÉ tr√°nh qu√° d√†i
                            if len(contexts) >= 4:
                                break

        except Exception as e:
            logger.warning(f"Error in RAG chat contexts retrieval: {e}")

            # Fallback: s·ª≠ d·ª•ng similarity search ƒë∆°n gi·∫£n
            try:
                docs = await self.master_chatbot.rag_chat.vector_store.similarity_search(question, k=3)
                contexts = [doc.page_content for doc in docs]
            except Exception as fallback_e:
                logger.error(f"Fallback context retrieval failed: {fallback_e}")
                contexts = [f"Context retrieval failed: {str(fallback_e)}"]

        return contexts

    async def evaluate_single_question(self, question: str, expected_answer: str, index: int) -> Dict:
        """
        ƒê√°nh gi√° m·ªôt c√¢u h·ªèi ƒë∆°n l·∫ª v·ªõi RAGAS
        """
        print(f"\nüîç Evaluating question {index + 1}: {question[:50]}...")

        # Generate RAG response
        rag_result = await self.generate_rag_response(question, f"eval_session_{index}")

        # Prepare data for RAGAS evaluation
        eval_data = {
            'question': [question],
            'answer': [rag_result['answer']],
            'ground_truth': [expected_answer],
            'contexts': [rag_result['contexts']]
        }

        result = {
            'question_index': index + 1,
            'question': question,
            'expected_answer': expected_answer,
            'generated_answer': rag_result['answer'],
            'latency': rag_result['latency'],
            'route_used': rag_result['route_used'],
            'contexts': rag_result['contexts']
        }

        # Evaluate with RAGAS
        try:
            # Convert to Dataset
            dataset = Dataset.from_dict(eval_data)

            # Wrap embeddings
            embeddings_wrapper = LangchainEmbeddingsWrapper(
                self.doc_processor.vector_store.embeddings
            )

            # Run evaluation
            evaluation_result = evaluate(
                dataset=dataset,
                metrics=RAGAS_METRICS,
                llm=self.llm_wrapper,
                embeddings=embeddings_wrapper
            )

            # Extract scores - ch·ªâ 2 metrics: answer_similarity, faithfulness
            if hasattr(evaluation_result, 'to_pandas'):
                eval_df = evaluation_result.to_pandas()
                print(eval_df)
                if len(eval_df) > 0:
                    first_row = eval_df.iloc[0]
                    result.update({
                        'answer_similarity': float(first_row.get('semantic_similarity', 0.0)),
                        'faithfulness': float(first_row.get('faithfulness', 0.0))
                    })
                else:
                    # Set default values if no results
                    result.update({
                        'answer_similarity': 0.0,
                        'faithfulness': 0.0
                    })
            else:
                # Fallback for different RAGAS return format
                result.update({
                    'answer_similarity': float(evaluation_result.get('semantic_similarity', 0.0)),
                    'faithfulness': float(evaluation_result.get('faithfulness', 0.0))
                })

            print(f"‚úÖ Question {index + 1} evaluated successfully")
            print(f"   Latency: {rag_result['latency']:.3f}s")
            print(f"   Route: {rag_result['route_used']}")
            print(f"   Answer Similarity: {result.get('answer_similarity', 0):.3f}")
            print(f"   Faithfulness: {result.get('faithfulness', 0):.3f}")

        except Exception as e:
            logger.error(f"Error in RAGAS evaluation for question {index + 1}: {e}")
            result.update({
                'answer_similarity': 0.0,
                'faithfulness': 0.0,
                'evaluation_error': str(e)
            })

        return result

    async def run_evaluation(self, testcase_file: str = "testcase.xlsx"):
        """
        Ch·∫°y ƒë√°nh gi√° to√†n b·ªô testcase
        """
        print("üöÄ Starting RAGAS Evaluation...")

        # Initialize chatbot
        await self.initialize_chatbot()

        # Load test data
        df = self.load_testcase_data(testcase_file)

        # Run evaluation for each question
        evaluation_results = []

        for index, row in df.iterrows():
            question = str(row['question']).strip()
            expected_answer = str(row['answer']).strip()

            if not question or question.lower() in ['nan', 'none', '']:
                print(f"‚ö†Ô∏è Skipping empty question at index {index}")
                continue

            result = await self.evaluate_single_question(question, expected_answer, index)
            evaluation_results.append(result)

            # Store latency for statistics
            self.latency_results.append(result['latency'])

            # Delay to avoid overwhelming the system
            await asyncio.sleep(EVAL_SETTINGS['delay_between_requests'])

        self.results = evaluation_results
        return evaluation_results

    def calculate_statistics(self) -> Dict:
        """
        T√≠nh to√°n th·ªëng k√™ t·ªïng h·ª£p - ch·ªâ 2 metrics: answer_similarity, faithfulness
        """
        if not self.results:
            return {}

        # Extract numeric metrics - ch·ªâ 3 metrics ch√≠nh (s·ª≠a t√™n metric ƒë√∫ng)
        metrics = ['answer_similarity', 'faithfulness']

        stats = {
            'total_questions': len(self.results),
            'successful_evaluations': len([r for r in self.results if 'evaluation_error' not in r]),
            'average_metrics': {},
            'latency_statistics': {},
            'quality_assessment': {}
        }

        # Calculate average metrics - INCLUDE zeros trong calculation
        for metric in metrics:
            # L·∫•y t·∫•t c·∫£ values bao g·ªìm c·∫£ 0 (nh∆∞ng exclude NaN v√† null)
            values = [r.get(metric, 0) for r in self.results if isinstance(r.get(metric), (int, float))]

            if values:
                mean_val = np.mean(values)
                stats['average_metrics'][metric] = {
                    'mean': mean_val,
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'count': len(values),
                    'zero_count': len([v for v in values if v == 0.0]),  # ƒê·∫øm s·ªë l∆∞·ª£ng 0
                    'non_zero_count': len([v for v in values if v > 0.0])  # ƒê·∫øm s·ªë l∆∞·ª£ng > 0
                }

                # Quality assessment
                thresholds = QUALITY_THRESHOLDS.get(metric, {})
                if mean_val >= thresholds.get('excellent', 0.8):
                    quality = 'Excellent'
                elif mean_val >= thresholds.get('good', 0.6):
                    quality = 'Good'
                elif mean_val >= thresholds.get('acceptable', 0.4):
                    quality = 'Acceptable'
                else:
                    quality = 'Needs Improvement'

                stats['quality_assessment'][metric] = quality
            else:
                # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ values n√†o
                stats['average_metrics'][metric] = {
                    'mean': 0.0,
                    'std': 0.0,
                    'min': 0.0,
                    'max': 0.0,
                    'count': 0,
                    'zero_count': 0,
                    'non_zero_count': 0
                }
                stats['quality_assessment'][metric] = 'No Data'

        # Calculate latency statistics
        if self.latency_results:
            stats['latency_statistics'] = {
                'mean': np.mean(self.latency_results),
                'std': np.std(self.latency_results),
                'min': np.min(self.latency_results),
                'max': np.max(self.latency_results),
                'p50': np.percentile(self.latency_results, 50),
                'p99': np.percentile(self.latency_results, 99)
            }

        return stats

    def export_results(self, output_file: str = None):
        """
        Xu·∫•t k·∫øt qu·∫£ ra file Excel
        """
        if not self.results:
            print("‚ùå No results to export")
            return

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"ragas_evaluation_results_{timestamp}.xlsx"

        # Create DataFrame from results
        df_results = pd.DataFrame(self.results)

        # Calculate statistics
        stats = self.calculate_statistics()

        # Create statistics DataFrame
        stats_data = []

        # Add average metrics
        for metric, values in stats.get('average_metrics', {}).items():
            stats_data.append({
                'Metric': f"{metric}_mean",
                'Value': values['mean'],
                'Quality': stats['quality_assessment'].get(metric, 'N/A')
            })
            stats_data.append({
                'Metric': f"{metric}_std",
                'Value': values['std'],
                'Quality': ''
            })

        # Add latency statistics
        for stat_name, value in stats.get('latency_statistics', {}).items():
            stats_data.append({
                'Metric': f"latency_{stat_name}",
                'Value': value,
                'Quality': ''
            })

        df_stats = pd.DataFrame(stats_data)

        # Create summary info
        summary_info = [
            {'Info': 'Total Questions', 'Value': stats['total_questions']},
            {'Info': 'Successful Evaluations', 'Value': stats['successful_evaluations']},
            {'Info': 'Success Rate', 'Value': f"{(stats['successful_evaluations']/stats['total_questions']*100):.1f}%"},
            {'Info': 'Evaluation Date', 'Value': datetime.now().strftime("%Y-%m-%d %H:%M:%S")},
            {'Info': 'Model Used', 'Value': 'gpt-4o-mini (via MultiModelAPI)'},
            {'Info': 'RAG System', 'Value': 'MasterChatbot with Dual Vector Stores'}
        ]
        df_summary = pd.DataFrame(summary_info)

        # Export to Excel with multiple sheets
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            df_results.to_excel(writer, sheet_name='Detailed_Results', index=False)
            df_stats.to_excel(writer, sheet_name='Summary_Statistics', index=False)
            df_summary.to_excel(writer, sheet_name='Evaluation_Info', index=False)

        print(f"‚úÖ Results exported to: {output_file}")

        # Print summary
        print("\nüìä EVALUATION SUMMARY:")
        print(f"Total Questions: {stats['total_questions']}")
        print(f"Successful Evaluations: {stats['successful_evaluations']}")
        print(f"Success Rate: {(stats['successful_evaluations']/stats['total_questions']*100):.1f}%")

        if 'average_metrics' in stats:
            print("\nüìà Average Metrics:")
            for metric, values in stats['average_metrics'].items():
                quality = stats['quality_assessment'].get(metric, 'N/A')
                print(f"  {metric}: {values['mean']:.4f} (¬±{values['std']:.4f}) - {quality}")

        if 'latency_statistics' in stats:
            print(f"\n‚è±Ô∏è Latency Statistics:")
            latency_stats = stats['latency_statistics']
            print(f"  Mean: {latency_stats['mean']:.3f}s")
            print(f"  P50: {latency_stats['p50']:.3f}s")
            print(f"  P99: {latency_stats['p99']:.3f}s")
            print(f"  Min: {latency_stats['min']:.3f}s")
            print(f"  Max: {latency_stats['max']:.3f}s")

async def main():
    """
    Main function ƒë·ªÉ ch·∫°y evaluation
    """
    evaluator = RAGASEvaluator()

    try:
        # Run evaluation
        results = await evaluator.run_evaluation("testcase.xlsx")

        # Export results
        evaluator.export_results()

        print("\nüéâ Evaluation completed successfully!")

    except Exception as e:
        logger.error(f"Error in main evaluation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
