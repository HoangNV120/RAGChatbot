from typing import Dict, Optional, List, Tuple
import logging
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from app.config import settings
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import asyncio

logger = logging.getLogger(__name__)

class CategoryPartitionedRouter:
    """
    Advanced Hybrid Router v·ªõi category-based data partitioning
    
    Lu·ªìng ho·∫°t ƒë·ªông:
    1. LLM Classification ƒë·ªÉ x√°c ƒë·ªãnh category
    2. N·∫øu category = "KH√ÅC" ‚Üí RAG_CHAT (b·ªè qua vector)
    3. N·∫øu category h·ª£p l·ªá ‚Üí Vector search CH·ªà trong partition t∆∞∆°ng ·ª©ng
    4. T√¨m similarity trong partition, n·∫øu ƒë·ªß cao ‚Üí tr·∫£ answer, n·∫øu kh√¥ng ‚Üí RAG_CHAT
    """
    
    def __init__(self, use_categorized_data=True):
        # Initialize LLM cho classification
        self.llm = ChatOpenAI(
            openai_api_key=settings.openai_api_key,
            model_name="gpt-3.5-turbo",
            temperature=0
        )
        
        # Initialize embeddings cho vector search
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.openai_api_key,
            model="text-embedding-ada-002"
        )
        
        # Load data - ∆∞u ti√™n data c√≥ category
        self.use_categorized_data = use_categorized_data
        self.data = self._load_data()
        
        # Similarity threshold cho vector search
        self.similarity_threshold = 0.9
        
        # ƒê·ªãnh nghƒ©a c√°c category h·ª£p l·ªá
        self.valid_categories = [
            "H·ªåC PH√ç", "NG√ÄNH H·ªåC", "QUY CH·∫æ THI", "ƒêI·ªÇM S·ªê", 
            "D·ªäCH V·ª§ SINH VI√äN", "C∆† S·ªû V·∫¨T CH·∫§T", "CH∆Ø∆†NG TR√åNH H·ªåC"
        ]
        
        # Cache cho embeddings theo category
        self.category_embeddings = {}  # {category: numpy_array}
        self.category_questions_data = {}  # {category: [question_data]}
        
        # Cache cho classification results ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô v√† consistency
        self.classification_cache = {}  # {query_hash: classification_result}
        
        print(f"CategoryPartitionedRouter initialized with {len(self.data)} questions")
        print(f"Valid categories: {self.valid_categories}")
        
        # Hi·ªÉn th·ªã th·ªëng k√™ category
        if 'category' in self.data.columns:
            category_stats = self.data['category'].value_counts()
            print(f"üìä Category distribution:")
            for cat, count in category_stats.items():
                print(f"   {cat}: {count} questions")
        else:
            print("‚ö†Ô∏è  No category column found - will categorize on demand")
    
    def _load_data(self):
        """Load data v·ªõi ∆∞u ti√™n file c√≥ category"""
        try:
            if self.use_categorized_data:
                # Th·ª≠ load file ƒë√£ c√≥ category tr∆∞·ªõc
                try:
                    df = pd.read_excel('app/data_test_with_categories.xlsx')
                    print("‚úÖ Loaded categorized data file")
                    return df
                except FileNotFoundError:
                    print("‚ö†Ô∏è  Categorized data file not found, falling back to original")
            
            # Fallback sang file g·ªëc
            df = pd.read_excel('app/data_test.xlsx')
            print("‚úÖ Loaded original data file")
            return df
            
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            raise
    
    async def _classify_query_category(self, query: str) -> Dict:
        """B∆∞·ªõc 1: Ph√¢n lo·∫°i category b·∫±ng LLM v·ªõi Prompt Engineering cao c·∫•p"""
        
        # üöÄ CACHE CHECK ƒë·ªÉ ƒë·∫£m b·∫£o consistency
        import hashlib
        query_hash = hashlib.md5(query.lower().strip().encode()).hexdigest()
        
        if query_hash in self.classification_cache:
            cached_result = self.classification_cache[query_hash]
            print(f"üéØ Using cached classification: {cached_result['category']}")
            return cached_result
        
        system_prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i c√¢u h·ªèi v·ªÅ tr∆∞·ªùng ƒë·∫°i h·ªçc v·ªõi ƒë·ªô ch√≠nh x√°c 100%. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n lo·∫°i c√¢u h·ªèi v√†o ƒê√öNG M·ªòT trong c√°c danh m·ª•c sau:

üìö DANH M·ª§C C·ª§ TH·ªÇ (7 lo·∫°i):
1. H·ªåC PH√ç - T·∫•t c·∫£ v·ªÅ ti·ªÅn b·∫°c:
   ‚Ä¢ H·ªçc ph√≠, chi ph√≠ h·ªçc t·∫≠p, ti·ªÅn ƒë√≥ng h·ªçc
   ‚Ä¢ Mi·ªÖn gi·∫£m h·ªçc ph√≠, h·ªçc b·ªïng
   ‚Ä¢ T·ª´ kh√≥a: "h·ªçc ph√≠", "chi ph√≠", "ti·ªÅn", "ƒë√≥ng h·ªçc", "mi·ªÖn gi·∫£m", "h·ªçc b·ªïng"

2. NG√ÄNH H·ªåC - V·ªÅ chuy√™n ng√†nh:
   ‚Ä¢ C√°c ng√†nh h·ªçc, chuy√™n ng√†nh, khoa
   ‚Ä¢ Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, li√™n th√¥ng
   ‚Ä¢ T·ª´ kh√≥a: "ng√†nh", "chuy√™n ng√†nh", "khoa", "ƒë√†o t·∫°o", "li√™n th√¥ng"

3. QUY CH·∫æ THI - V·ªÅ thi c·ª≠ v√† t·ªët nghi·ªáp:
   ‚Ä¢ Quy ƒë·ªãnh thi, ƒëi·ªÅu ki·ªán thi, l·ªãch thi
   ‚Ä¢ ƒêi·ªÅu ki·ªán t·ªët nghi·ªáp, quy ch·∫ø h·ªçc v·ª•
   ‚Ä¢ T·ª´ kh√≥a: "thi", "ki·ªÉm tra", "t·ªët nghi·ªáp", "quy ch·∫ø", "ƒëi·ªÅu ki·ªán"

4. ƒêI·ªÇM S·ªê - V·ªÅ ƒëi·ªÉm v√† ƒë√°nh gi√°:
   ‚Ä¢ Thang ƒëi·ªÉm, c√°ch t√≠nh ƒëi·ªÉm, GPA
   ‚Ä¢ X·∫øp lo·∫°i, h·ªçc l·ª±c, ƒëi·ªÉm trung b√¨nh
   ‚Ä¢ T·ª´ kh√≥a: "ƒëi·ªÉm", "GPA", "thang ƒëi·ªÉm", "x·∫øp lo·∫°i", "h·ªçc l·ª±c"

5. D·ªäCH V·ª§ SINH VI√äN - V·ªÅ th·ªß t·ª•c v√† h·ªó tr·ª£:
   ‚Ä¢ Th·ªß t·ª•c h√†nh ch√≠nh, ƒëƒÉng k√Ω h·ªçc ph·∫ßn
   ‚Ä¢ D·ªãch v·ª• h·ªó tr·ª£, t∆∞ v·∫•n sinh vi√™n
   ‚Ä¢ T·ª´ kh√≥a: "ƒëƒÉng k√Ω", "th·ªß t·ª•c", "h·ªó tr·ª£", "d·ªãch v·ª•", "t∆∞ v·∫•n"

6. C∆† S·ªû V·∫¨T CH·∫§T - V·ªÅ kh√¥ng gian v·∫≠t l√Ω:
   ‚Ä¢ Ph√≤ng h·ªçc, th∆∞ vi·ªán, k√Ω t√∫c x√°
   ‚Ä¢ C∆° s·ªü v·∫≠t ch·∫•t, trang thi·∫øt b·ªã
   ‚Ä¢ T·ª´ kh√≥a: "ph√≤ng", "th∆∞ vi·ªán", "k√Ω t√∫c x√°", "c∆° s·ªü", "trang thi·∫øt b·ªã"

7. CH∆Ø∆†NG TR√åNH H·ªåC - V·ªÅ m√¥n h·ªçc v√† l·ªãch h·ªçc:
   ‚Ä¢ M√¥n h·ªçc, t√≠n ch·ªâ, th·ªùi kh√≥a bi·ªÉu
   ‚Ä¢ L·ªãch h·ªçc, l·ªãch thi, khung ch∆∞∆°ng tr√¨nh
   ‚Ä¢ T·ª´ kh√≥a: "m√¥n h·ªçc", "t√≠n ch·ªâ", "l·ªãch h·ªçc", "th·ªùi kh√≥a bi·ªÉu"

üö´ KH√ÅC - Kh√¥ng li√™n quan gi√°o d·ª•c:
   ‚Ä¢ Th·ªùi ti·∫øt, n·∫•u ƒÉn, th·ªÉ thao, gi·∫£i tr√≠
   ‚Ä¢ Tin t·ª©c, c√¥ng ngh·ªá kh√¥ng li√™n quan h·ªçc t·∫≠p
   ‚Ä¢ C√¢u h·ªèi c√° nh√¢n kh√¥ng v·ªÅ tr∆∞·ªùng h·ªçc

üéØ QUY TR√åNH PH√ÇN LO·∫†I (Th·ª±c hi·ªán tu·∫ßn t·ª±):

B∆Ø·ªöC 1: X√°c ƒë·ªãnh t·ª´ kh√≥a ch√≠nh trong c√¢u h·ªèi
B∆Ø·ªöC 2: ƒê·ªëi chi·∫øu v·ªõi 7 danh m·ª•c tr√™n (∆∞u ti√™n theo th·ª© t·ª±)
B∆Ø·ªöC 3: N·∫øu kh√¥ng kh·ªõp ‚Üí ch·ªçn "KH√ÅC"
B∆Ø·ªöC 4: Double-check k·∫øt qu·∫£

‚ö†Ô∏è QUY T·∫ÆC V√ÄNG:
- CH·ªà tr·∫£ v·ªÅ T√äN DANH M·ª§C (vi·∫øt hoa, kh√¥ng d·∫•u ph·∫©y, kh√¥ng gi·∫£i th√≠ch)
- N·∫øu c√≥ th·ªÉ thu·ªôc 2 danh m·ª•c ‚Üí ch·ªçn danh m·ª•c ch√≠nh y·∫øu nh·∫•t
- Khi nghi ng·ªù ‚Üí ch·ªçn "KH√ÅC"
- KH√îNG bao gi·ªù t·ª± t·∫°o danh m·ª•c m·ªõi

üìù M·∫™U PH·∫¢N H·ªíI ƒê√öNG:
- Input: "H·ªçc ph√≠ ng√†nh CNTT?" ‚Üí Output: "H·ªåC PH√ç"
- Input: "Th·ªùi ti·∫øt h√¥m nay?" ‚Üí Output: "KH√ÅC"
- Input: "L·ªãch thi cu·ªëi k·ª≥?" ‚Üí Output: "QUY CH·∫æ THI"

CH·ªà TR·∫¢ V·ªÄ T√äN DANH M·ª§C DUY NH·∫§T - KH√îNG GI·∫¢I TH√çCH G√å TH√äM."""

        try:
            # üéØ FEW-SHOT EXAMPLES ƒë·ªÉ tƒÉng consistency
            few_shot_examples = [
                ("H·ªçc ph√≠ ng√†nh C√¥ng ngh·ªá th√¥ng tin bao nhi√™u?", "H·ªåC PH√ç"),
                ("C√°c ng√†nh h·ªçc t·∫°i tr∆∞·ªùng c√≥ g√¨?", "NG√ÄNH H·ªåC"),
                ("ƒêi·ªÅu ki·ªán thi t·ªët nghi·ªáp l√† g√¨?", "QUY CH·∫æ THI"),
                ("Thang ƒëi·ªÉm t·∫°i tr∆∞·ªùng nh∆∞ th·∫ø n√†o?", "ƒêI·ªÇM S·ªê"),
                ("L√†m sao ƒë·ªÉ ƒëƒÉng k√Ω h·ªçc ph·∫ßn?", "D·ªäCH V·ª§ SINH VI√äN"),
                ("Th∆∞ vi·ªán tr∆∞·ªùng c√≥ m·ªü c·ª≠a kh√¥ng?", "C∆† S·ªû V·∫¨T CH·∫§T"),
                ("L·ªãch h·ªçc m√¥n To√°n l√† g√¨?", "CH∆Ø∆†NG TR√åNH H·ªåC"),
                ("H√¥m nay tr·ªùi ƒë·∫πp qu√°!", "KH√ÅC")
            ]
            
            # T·∫°o few-shot prompt
            few_shot_prompt = "\n".join([
                f"V√≠ d·ª•: \"{ex[0]}\" ‚Üí {ex[1]}" for ex in few_shot_examples
            ])
            
            # üß† ENHANCED PROMPT v·ªõi few-shot v√† reasoning
            enhanced_system_prompt = system_prompt + f"""

üéì C√ÅC V√ç D·ª§ CHU·∫®N (h·ªçc t·ª´ c√°c tr∆∞·ªùng h·ª£p n√†y):
{few_shot_prompt}

üîç QUY TR√åNH T∆íRANG LOGIC:
1. ƒê·ªçc c√¢u h·ªèi ‚Üí T√¨m t·ª´ kh√≥a ch√≠nh
2. So s√°nh v·ªõi 8 v√≠ d·ª• tr√™n ‚Üí T√¨m m·∫´u t∆∞∆°ng t·ª±
3. √Åp d·ª•ng logic t∆∞∆°ng t·ª± ‚Üí ƒê∆∞a ra quy·∫øt ƒë·ªãnh
4. Ki·ªÉm tra l·∫°i ‚Üí ƒê·∫£m b·∫£o ƒë√∫ng format

‚ö° L∆ØU √ù QUAN TR·ªåNG:
- Ph√¢n t√≠ch CH√çNH X√ÅC nh∆∞ c√°c v√≠ d·ª• tr√™n
- Ph·∫£n h·ªìi ƒê·ªíNG NH·∫§T v·ªõi training pattern
- KH√îNG thay ƒë·ªïi c√°ch ph√¢n lo·∫°i so v·ªõi examples"""

            messages = [
                SystemMessage(content=enhanced_system_prompt),
                HumanMessage(content=f"Ph√¢n lo·∫°i c√¢u h·ªèi: \"{query}\"")
            ]
            
            # üöÄ DOUBLE-CHECK CLASSIFICATION v·ªõi temperature=0 ƒë·ªÉ consistency
            response = await self.llm.agenerate([messages])
            category = response.generations[0][0].text.strip().upper()
            
            # üéØ VALIDATION & NORMALIZATION
            # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng mong mu·ªën
            category = category.replace(".", "").replace(",", "").replace(":", "").strip()
            
            # üìù KEYWORD-BASED FALLBACK ƒë·ªÉ ƒë·∫£m b·∫£o consistency
            query_lower = query.lower()
            keyword_mapping = {
                "H·ªåC PH√ç": ["h·ªçc ph√≠", "chi ph√≠", "ti·ªÅn", "ƒë√≥ng h·ªçc", "mi·ªÖn gi·∫£m", "h·ªçc b·ªïng", "ph√≠"],
                "NG√ÄNH H·ªåC": ["ng√†nh", "chuy√™n ng√†nh", "khoa", "ƒë√†o t·∫°o", "li√™n th√¥ng", "chuy√™n m√¥n"],
                "QUY CH·∫æ THI": ["thi", "ki·ªÉm tra", "t·ªët nghi·ªáp", "quy ch·∫ø", "ƒëi·ªÅu ki·ªán", "quy ƒë·ªãnh"],
                "ƒêI·ªÇM S·ªê": ["ƒëi·ªÉm", "gpa", "thang ƒëi·ªÉm", "x·∫øp lo·∫°i", "h·ªçc l·ª±c", "ƒë√°nh gi√°"],
                "D·ªäCH V·ª§ SINH VI√äN": ["ƒëƒÉng k√Ω", "th·ªß t·ª•c", "h·ªó tr·ª£", "d·ªãch v·ª•", "t∆∞ v·∫•n", "h√†nh ch√≠nh"],
                "C∆† S·ªû V·∫¨T CH·∫§T": ["ph√≤ng", "th∆∞ vi·ªán", "k√Ω t√∫c x√°", "c∆° s·ªü", "trang thi·∫øt b·ªã", "khu√¥n vi√™n"],
                "CH∆Ø∆†NG TR√åNH H·ªåC": ["m√¥n h·ªçc", "t√≠n ch·ªâ", "l·ªãch h·ªçc", "th·ªùi kh√≥a bi·ªÉu", "ch∆∞∆°ng tr√¨nh", "khung"]
            }
            
            # üîç CONSISTENCY CHECK: So s√°nh LLM result v·ªõi keyword matching
            keyword_category = None
            max_matches = 0
            
            for cat, keywords in keyword_mapping.items():
                matches = sum(1 for keyword in keywords if keyword in query_lower)
                if matches > max_matches:
                    max_matches = matches
                    keyword_category = cat
            
            # üéØ FINAL DECISION v·ªõi priority rules
            final_category = category
            
            # Rule 1: N·∫øu LLM classification kh√¥ng h·ª£p l·ªá, d√πng keyword fallback
            if category not in self.valid_categories and category != "KH√ÅC":
                if keyword_category and max_matches >= 1:
                    final_category = keyword_category
                    print(f"üîÑ LLM classification '{category}' invalid, using keyword fallback: '{keyword_category}'")
                else:
                    final_category = "KH√ÅC"
                    print(f"üîÑ LLM classification '{category}' invalid, no keywords found ‚Üí KH√ÅC")
            
            # Rule 2: Cross-validation - n·∫øu c√≥ conflict m·∫°nh, ∆∞u ti√™n keyword
            elif keyword_category and keyword_category != category and max_matches >= 2:
                print(f"üéØ Strong keyword evidence for '{keyword_category}' vs LLM '{category}', using keywords")
                final_category = keyword_category
            
            # üìä LOG DECISION PROCESS
            print(f"üè∑Ô∏è  Classification result:")
            print(f"   LLM: {category}")
            print(f"   Keywords: {keyword_category} (matches: {max_matches})")
            print(f"   Final: {final_category}")
            
            # Ki·ªÉm tra category c√≥ h·ª£p l·ªá kh√¥ng
            result = None
            if final_category in self.valid_categories:
                result = {
                    "category": final_category,
                    "is_valid": True,
                    "should_use_vector": True,
                    "llm_category": category,
                    "keyword_category": keyword_category,
                    "keyword_matches": max_matches
                }
            else:
                result = {
                    "category": "KH√ÅC",
                    "is_valid": False,
                    "should_use_vector": False,
                    "llm_category": category,
                    "keyword_category": keyword_category,
                    "keyword_matches": max_matches
                }
            
            # üíæ CACHE RESULT ƒë·ªÉ consistency
            self.classification_cache[query_hash] = result
            
            return result
                
        except Exception as e:
            logger.error(f"Error in LLM classification: {e}")
            print(f"‚ùå LLM classification error: {e}")
            # Fallback: coi nh∆∞ category KH√ÅC
            return {
                "category": "KH√ÅC",
                "is_valid": False,
                "should_use_vector": False,
                "error": str(e)
            }
    
    async def _get_or_assign_category(self, question: str, row_index: int) -> str:
        """L·∫•y category t·ª´ data ho·∫∑c g√°n m·ªõi n·∫øu ch∆∞a c√≥ (t·ªëi ∆∞u h√≥a)"""
        
        # N·∫øu data ƒë√£ c√≥ category column v√† c√≥ gi√° tr·ªã h·ª£p l·ªá
        if 'category' in self.data.columns:
            existing_category = self.data.iloc[row_index].get('category', '')
            if pd.notna(existing_category) and str(existing_category).strip():
                category = str(existing_category).strip().upper()
                # Ki·ªÉm tra category c√≥ trong danh s√°ch h·ª£p l·ªá kh√¥ng
                if category in self.valid_categories:
                    return category
                elif category == "KH√ÅC":
                    return "KH√ÅC"
        
        # N·∫øu ch∆∞a c√≥ ho·∫∑c kh√¥ng h·ª£p l·ªá, ph√¢n lo·∫°i b·∫±ng LLM (ch·∫≠m)
        print(f"üîÑ Auto-categorizing question: {question[:50]}...")
        classification = await self._classify_query_category(question)
        return classification["category"]
    
    async def _initialize_category_embeddings(self, target_category: str):
        """Kh·ªüi t·∫°o embeddings cho m·ªôt category c·ª• th·ªÉ"""
        
        if target_category in self.category_embeddings:
            return  # ƒê√£ kh·ªüi t·∫°o r·ªìi
        
        print(f"üîÑ Initializing embeddings for category: {target_category}")
        
        # L·ªçc v√† chu·∫©n b·ªã c√¢u h·ªèi cho category n√†y
        category_questions = []
        category_data = []
        
        for idx, row in self.data.iterrows():
            question = str(row['question']).strip()
            answer = str(row['answer']).strip()
            source = str(row.get('ngu·ªìn', '')).strip()
            
            if not question or not answer:
                continue
            
            # L·∫•y ho·∫∑c g√°n category
            row_category = await self._get_or_assign_category(question, idx)
            
            # Ch·ªâ l·∫•y c√¢u h·ªèi thu·ªôc category n√†y
            if row_category == target_category:
                # X·ª≠ l√Ω multiple questions (n·∫øu c√≥ d·∫•u |)
                question_parts = [q.strip() for q in question.split('|') if q.strip()]
                
                for q in question_parts:
                    category_questions.append(q)
                    category_data.append({
                        'original_index': idx,
                        'question': q,
                        'answer': answer,
                        'source': source,
                        'category': row_category
                    })
        
        if not category_questions:
            print(f"‚ö†Ô∏è  No questions found for category: {target_category}")
            self.category_embeddings[target_category] = np.array([])
            self.category_questions_data[target_category] = []
            return
        
        print(f"üìä Processing {len(category_questions)} questions for category {target_category}")
        
        # T·∫°o embeddings cho category n√†y
        try:
            # Batch processing ƒë·ªÉ t·ªëi ∆∞u
            batch_size = 50
            all_embeddings = []
            
            for i in range(0, len(category_questions), batch_size):
                batch = category_questions[i:i + batch_size]
                print(f"   Processing batch {i//batch_size + 1}/{(len(category_questions) + batch_size - 1)//batch_size}")
                
                # Get embeddings for batch
                batch_embeddings = await self.embeddings.aembed_documents(batch)
                all_embeddings.extend(batch_embeddings)
                
                # Small delay to avoid rate limiting
                await asyncio.sleep(0.1)
            
            # Cache embeddings v√† data
            self.category_embeddings[target_category] = np.array(all_embeddings)
            self.category_questions_data[target_category] = category_data
            
            print(f"‚úÖ Created embeddings for {len(category_questions)} questions in category {target_category}")
            print(f"üìê Embedding dimension: {self.category_embeddings[target_category].shape[1]}")
            
        except Exception as e:
            logger.error(f"Error creating embeddings for category {target_category}: {e}")
            print(f"‚ùå Error creating embeddings for category {target_category}: {e}")
            raise
    
    async def _get_query_embedding(self, query: str) -> np.ndarray:
        """L·∫•y embedding cho c√¢u h·ªèi input"""
        try:
            embedding = await self.embeddings.aembed_query(query)
            return np.array(embedding)
        except Exception as e:
            logger.error(f"Error getting query embedding: {e}")
            raise
    
    def _find_most_similar_in_category(self, query_embedding: np.ndarray, category: str, top_k: int = 5) -> List[Tuple[int, float, Dict]]:
        """T√¨m c√¢u h·ªèi t∆∞∆°ng t·ª± nh·∫•t trong m·ªôt category c·ª• th·ªÉ"""
        
        if category not in self.category_embeddings or len(self.category_embeddings[category]) == 0:
            return []
        
        category_embed = self.category_embeddings[category]
        category_data = self.category_questions_data[category]
        
        # T√≠nh cosine similarity ch·ªâ trong category n√†y
        similarities = cosine_similarity([query_embedding], category_embed)[0]
        
        # L·∫•y top_k c√¢u h·ªèi c√≥ similarity cao nh·∫•t
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            similarity_score = similarities[idx]
            question_data = category_data[idx]
            
            results.append((idx, similarity_score, question_data))
        
        return results
    
    async def _vector_search_in_category(self, query: str, category: str) -> Dict:
        """B∆∞·ªõc 3: Vector search trong category partition c·ª• th·ªÉ"""
        
        # Kh·ªüi t·∫°o embeddings cho category n√†y
        await self._initialize_category_embeddings(category)
        
        print(f"üîç Vector search in category '{category}' for query: {query[:50]}...")
        
        # Ki·ªÉm tra c√≥ data cho category n√†y kh√¥ng
        if category not in self.category_questions_data or len(self.category_questions_data[category]) == 0:
            print(f"‚ùå No data found for category: {category}")
            return {
                "route": "RAG_CHAT",
                "reason": f"No questions found in category {category}",
                "similarity_score": 0.0
            }
        
        # L·∫•y embedding cho c√¢u h·ªèi input
        query_embedding = await self._get_query_embedding(query)
        
        # T√¨m c√¢u h·ªèi t∆∞∆°ng t·ª± nh·∫•t trong category n√†y
        similar_questions = self._find_most_similar_in_category(query_embedding, category, top_k=3)
        
        if not similar_questions:
            print(f"‚ùå No similar questions found in category {category}")
            return {
                "route": "RAG_CHAT",
                "reason": f"No similar questions in category {category}",
                "similarity_score": 0.0
            }
        
        # L·∫•y c√¢u h·ªèi c√≥ similarity cao nh·∫•t
        best_idx, best_similarity, best_question_data = similar_questions[0]
        
        print(f"üìä Best similarity in category '{category}': {best_similarity:.3f}")
        print(f"üîç Best match: {best_question_data['question'][:50]}...")
        
        # Quy·∫øt ƒë·ªãnh d·ª±a tr√™n threshold
        if best_similarity >= self.similarity_threshold:
            print(f"‚úÖ Found good match in category '{category}' (similarity: {best_similarity:.3f} >= {self.similarity_threshold})")
            
            return {
                "route": "VECTOR_BASED",
                "similarity_score": best_similarity,
                "matched_question": best_question_data['question'],
                "answer": best_question_data['answer'],
                "source": best_question_data['source'],
                "matched_category": category,
                "all_matches": [
                    {
                        "question": q_data['question'],
                        "similarity": sim,
                        "answer": q_data['answer'][:100] + "..." if len(q_data['answer']) > 100 else q_data['answer'],
                        "category": category
                    }
                    for _, sim, q_data in similar_questions
                ]
            }
        else:
            print(f"‚ùå Similarity too low in category '{category}' (best: {best_similarity:.3f} < {self.similarity_threshold})")
            
            return {
                "route": "RAG_CHAT",
                "reason": f"Best similarity {best_similarity:.3f} in category '{category}' below threshold {self.similarity_threshold}",
                "similarity_score": best_similarity,
                "searched_category": category,
                "best_match": {
                    "question": best_question_data['question'],
                    "similarity": best_similarity,
                    "answer": best_question_data['answer'][:100] + "..." if len(best_question_data['answer']) > 100 else best_question_data['answer'],
                    "category": category
                }
            }
    
    async def route_query(self, query: str) -> Dict:
        """Main routing function v·ªõi category-partitioned approach"""
        try:
            print(f"\nüöÄ Category-partitioned routing for query: {query[:50]}...")
            
            # B∆∞·ªõc 1: LLM Classification
            classification_result = await self._classify_query_category(query)
            category = classification_result["category"]
            should_use_vector = classification_result["should_use_vector"]
            
            # B∆∞·ªõc 2: Quy·∫øt ƒë·ªãnh lu·ªìng d·ª±a tr√™n category
            if not should_use_vector or category == "KH√ÅC":
                print(f"‚ö° Category '{category}' ‚Üí Skip vector search ‚Üí Direct RAG_CHAT")
                return {
                    "route": "RAG_CHAT",
                    "reason": f"Category '{category}' requires full RAG processing",
                    "query": query,
                    "classification": classification_result,
                    "similarity_score": 0.0
                }
            
            # B∆∞·ªõc 3: Vector Search trong category partition c·ª• th·ªÉ
            print(f"üéØ Category '{category}' ‚Üí Search in category partition...")
            vector_result = await self._vector_search_in_category(query, category)
            
            # Th√™m th√¥ng tin classification v√†o k·∫øt qu·∫£
            vector_result["query"] = query
            vector_result["classification"] = classification_result
            
            return vector_result
            
        except Exception as e:
            logger.error(f"Error in category-partitioned routing: {e}")
            print(f"‚ùå Error in category-partitioned routing: {e}")
            return {
                "route": "RAG_CHAT",
                "reason": f"Error during routing: {str(e)}",
                "query": query,
                "similarity_score": 0.0
            }
    
    def get_stats(self) -> Dict:
        """Th·ªëng k√™ v·ªÅ category-partitioned router"""
        
        total_questions = 0
        category_stats = {}
        
        for category in self.valid_categories:
            if category in self.category_questions_data:
                count = len(self.category_questions_data[category])
                category_stats[category] = count
                total_questions += count
            else:
                category_stats[category] = 0
        
        stats = {
            "total_questions": total_questions,
            "category_breakdown": category_stats,
            "similarity_threshold": self.similarity_threshold,
            "valid_categories": self.valid_categories,
            "partitions_initialized": list(self.category_embeddings.keys()),
            "use_categorized_data": self.use_categorized_data
        }
        
        return stats
    
    def set_similarity_threshold(self, threshold: float):
        """ƒêi·ªÅu ch·ªânh threshold cho vector search"""
        if 0.0 <= threshold <= 1.0:
            self.similarity_threshold = threshold
            print(f"üìä Vector similarity threshold updated to: {threshold}")
        else:
            print(f"‚ùå Invalid threshold: {threshold}. Must be between 0.0 and 1.0")
    
    async def test_query(self, query: str, show_details: bool = True) -> Dict:
        """Test m·ªôt c√¢u h·ªèi v·ªõi category-partitioned approach"""
        
        print(f"\nüß™ Testing category-partitioned routing for: '{query}'")
        
        # Test classification
        print(f"\nüìã Step 1: LLM Classification")
        classification_result = await self._classify_query_category(query)
        category = classification_result['category']
        print(f"   Category: {category}")
        print(f"   Should use vector: {classification_result['should_use_vector']}")
        
        if classification_result['should_use_vector'] and category != "KH√ÅC":
            print(f"\nüîç Step 2: Category-Partitioned Vector Search")
            
            # Initialize category embeddings
            await self._initialize_category_embeddings(category)
            
            if category in self.category_questions_data:
                category_count = len(self.category_questions_data[category])
                print(f"   üìä Searching in {category_count} questions in category '{category}'")
                
                # Get embedding v√† search
                query_embedding = await self._get_query_embedding(query)
                similar_questions = self._find_most_similar_in_category(query_embedding, category, top_k=5)
                
                if show_details and similar_questions:
                    print(f"\nüìä Top 5 similar questions in category '{category}':")
                    for i, (idx, similarity, q_data) in enumerate(similar_questions, 1):
                        print(f"  {i}. Similarity: {similarity:.3f}")
                        print(f"     Question: {q_data['question']}")
                        print(f"     Answer: {q_data['answer'][:100]}...")
                        print()
            else:
                print(f"   ‚ùå No data available for category '{category}'")
        else:
            print(f"\n‚ö° Step 2: Skipped vector search (Category: {category})")
        
        # Get final routing result
        print(f"\nüéØ Final Routing Result:")
        result = await self.route_query(query)
        
        print(f"   Route: {result['route']}")
        if result['route'] == "VECTOR_BASED":
            print(f"   ‚úÖ Matched answer: {result['answer'][:150]}...")
            print(f"   üìä Similarity: {result['similarity_score']:.3f}")
            print(f"   üè∑Ô∏è  Matched in category: {result.get('matched_category', 'N/A')}")
        else:
            print(f"   ‚ùå Reason: {result['reason']}")
        
        return result

    async def test_classification_consistency(self, query: str, num_tests: int = 5) -> Dict:
        """Test t√≠nh nh·∫•t qu√°n c·ªßa classification cho c√πng m·ªôt c√¢u h·ªèi"""
        
        print(f"\nüß™ Testing classification consistency for: '{query}'")
        print(f"üîÑ Running {num_tests} classification attempts...")
        
        results = []
        categories = []
        
        for i in range(num_tests):
            print(f"   Test {i+1}/{num_tests}...", end="")
            
            try:
                result = await self._classify_query_category(query)
                category = result["category"]
                categories.append(category)
                results.append(result)
                print(f" ‚Üí {category}")
                
                # Small delay to avoid rate limiting
                await asyncio.sleep(0.5)
                
            except Exception as e:
                print(f" ‚Üí ERROR: {e}")
                categories.append("ERROR")
        
        # Ph√¢n t√≠ch k·∫øt qu·∫£
        from collections import Counter
        category_counts = Counter(categories)
        total_valid = len([c for c in categories if c != "ERROR"])
        
        if total_valid == 0:
            return {
                "query": query,
                "consistency_rate": 0.0,
                "dominant_category": "ERROR",
                "all_results": categories,
                "is_consistent": False
            }
        
        # T√¨m category xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
        dominant_category = category_counts.most_common(1)[0][0]
        dominant_count = category_counts[dominant_category]
        consistency_rate = dominant_count / total_valid
        
        print(f"\nüìä Consistency Analysis:")
        print(f"   Dominant category: {dominant_category}")
        print(f"   Consistency rate: {consistency_rate:.1%} ({dominant_count}/{total_valid})")
        print(f"   All results: {dict(category_counts)}")
        
        is_consistent = consistency_rate >= 0.8  # 80% threshold
        consistency_status = "‚úÖ CONSISTENT" if is_consistent else "‚ùå INCONSISTENT"
        print(f"   Status: {consistency_status}")
        
        return {
            "query": query,
            "consistency_rate": consistency_rate,
            "dominant_category": dominant_category,
            "category_distribution": dict(category_counts),
            "all_results": categories,
            "is_consistent": is_consistent,
            "num_tests": num_tests
        }

    async def batch_test_consistency(self, test_queries: List[str], tests_per_query: int = 3) -> Dict:
        """Test t√≠nh nh·∫•t qu√°n cho nhi·ªÅu c√¢u h·ªèi"""
        
        print(f"\nüöÄ Batch testing classification consistency")
        print(f"üìù {len(test_queries)} queries, {tests_per_query} tests each")
        
        all_results = []
        consistent_count = 0
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nüìã Query {i}/{len(test_queries)}: {query[:50]}...")
            
            result = await self.test_classification_consistency(query, tests_per_query)
            all_results.append(result)
            
            if result["is_consistent"]:
                consistent_count += 1
        
        # T·ªïng k·∫øt
        overall_consistency = consistent_count / len(test_queries)
        
        print(f"\nüéØ OVERALL CONSISTENCY REPORT:")
        print(f"   Total queries tested: {len(test_queries)}")
        print(f"   Consistent queries: {consistent_count}")
        print(f"   Overall consistency rate: {overall_consistency:.1%}")
        
        # Ph√¢n t√≠ch category performance
        category_performance = {}
        for result in all_results:
            cat = result["dominant_category"]
            if cat not in category_performance:
                category_performance[cat] = []
            category_performance[cat].append(result["consistency_rate"])
        
        print(f"\nüìä Category Performance:")
        for cat, rates in category_performance.items():
            avg_rate = sum(rates) / len(rates)
            print(f"   {cat}: {avg_rate:.1%} avg consistency ({len(rates)} queries)")
        
        return {
            "overall_consistency_rate": overall_consistency,
            "consistent_queries": consistent_count,
            "total_queries": len(test_queries),
            "category_performance": {
                cat: sum(rates) / len(rates) 
                for cat, rates in category_performance.items()
            },
            "detailed_results": all_results
        }
    
    def clear_classification_cache(self):
        """X√≥a cache classification"""
        self.classification_cache.clear()
        print("üóëÔ∏è Classification cache cleared")
    
    def get_cache_stats(self) -> Dict:
        """Th·ªëng k√™ cache"""
        return {
            "cache_size": len(self.classification_cache),
            "cached_queries": list(self.classification_cache.keys())[:5]  # Show first 5 hashes
        }
    
    def save_classification_cache(self, filepath: str = "classification_cache.json"):
        """L∆∞u cache v√†o file ƒë·ªÉ persistent consistency"""
        import json
        try:
            # Convert ƒë·ªÉ serializable
            serializable_cache = {}
            for query_hash, result in self.classification_cache.items():
                serializable_cache[query_hash] = result
                
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(serializable_cache, f, ensure_ascii=False, indent=2)
            print(f"üíæ Classification cache saved to {filepath} ({len(serializable_cache)} entries)")
        except Exception as e:
            print(f"‚ùå Error saving cache: {e}")
    
    def load_classification_cache(self, filepath: str = "classification_cache.json"):
        """Load cache t·ª´ file ƒë·ªÉ maintain consistency"""
        import json
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                self.classification_cache = json.load(f)
            print(f"üì• Classification cache loaded from {filepath} ({len(self.classification_cache)} entries)")
            print("‚úÖ This ensures 100% classification consistency for cached queries!")
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  Cache file {filepath} not found, starting with empty cache")
        except Exception as e:
            print(f"‚ùå Error loading cache: {e}")
