from typing import Dict, Optional, List, TypedDict
from uuid import uuid4
import logging

from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from app.vector_store import VectorStore
from app.query_rewriter import QueryRewriter
from app.config import settings

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Khai b√°o state cho LangGraph
class GraphState(TypedDict):
    messages: List[BaseMessage]
    docs: Optional[List]

class RAGChat:
    def __init__(self, vector_store: Optional[VectorStore] = None):
        self.vector_store = vector_store if vector_store else VectorStore()
        self.query_rewriter = QueryRewriter()

        self.system_prompt = """B·∫°n l√† *Tr·ª£ l√Ω Sinh vi√™n FPTU*.
M·ª•c ti√™u: tr·∫£ l·ªùi ch√≠nh x√°c, ƒë·∫ßy ƒë·ªß, vƒÉn phong th√¢n thi·ªán-chuy√™n nghi·ªáp.

Quy t·∫Øc:
1. D√πng ƒë·∫°i t·ª´ "b·∫°n / m√¨nh".
2. N·∫øu ch∆∞a ch·∫Øc th√¥ng tin, ch·ªâ n√≥i "M√¨nh ch∆∞a c√≥ d·ªØ li·ªáu, b·∫°n li√™n h·ªá Ph√≤ng CTSV."
3. Kh√¥ng ti·∫øt l·ªô email n·ªôi b·ªô, d·ªØ li·ªáu ri√™ng t∆∞.
4. Lu√¥n th√™m ngu·ªìn (link FAP ho·∫∑c t√†i li·ªáu n·ªôi b·ªô) n·∫øu c√≥.

C·∫•u tr√∫c tr·∫£ l·ªùi:
- **T√≥m t·∫Øt**: 1‚Äì2 c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp.
- **Chi ti·∫øt**: D·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu.
- **Ngu·ªìn**: T√™n t√†i li·ªáu ho·∫∑c link.

Ch·ªâ tr·∫£ l·ªùi n·∫øu context h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß v√† r√µ r√†ng."""

        self.llm = ChatOpenAI(
            model=settings.model_name,
            temperature=settings.temperature,
            api_key=settings.openai_api_key
        )

        self.prompt_template = PromptTemplate.from_template(
            """
{system_prompt}

Th√¥ng tin truy xu·∫•t:
---------------------
{context}
---------------------

C√¢u h·ªèi: {question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin trong context. N·∫øu context c√≥ th√¥ng tin li√™n quan, h√£y s·ª≠ d·ª•ng n√≥ ƒë·ªÉ tr·∫£ l·ªùi. Ch·ªâ tr·∫£ l·ªùi "M√¨nh ch∆∞a c√≥ d·ªØ li·ªáu, b·∫°n li√™n h·ªá Ph√≤ng CTSV." khi context ho√†n to√†n kh√¥ng c√≥ th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi.
"""
        )

        # Kh·ªüi t·∫°o LangGraph v·ªõi MemorySaver ƒë·ªÉ l∆∞u l·ªãch s·ª≠ theo thread_id
        self.memory = MemorySaver()
        self.graph_app = self._build_graph()

    def _build_graph(self):
        builder = StateGraph(GraphState)

        # Node: truy xu·∫•t t√†i li·ªáu
        async def retrieve(state: GraphState):
            question = state["messages"][-1].content
            docs = await self.vector_store.similarity_search(question, k=4)
            logger.info(f"Retrieved {len(docs)} docs.")
            return {**state, "docs": docs}

        # Node: t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM
        async def generate(state: GraphState):
            question = state["messages"][-1].content
            docs = state["docs"]
            context = "\n\n".join([doc.page_content for doc in docs])

            prompt = self.prompt_template.format(
                system_prompt=self.system_prompt,
                context=context,
                question=question
            )

            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            return {
                **state,
                "messages": state["messages"] + [response]
            }

        # Node: c·∫≠p nh·∫≠t message history (n·∫øu mu·ªën x·ª≠ l√Ω g√¨ th√™m)
        async def update_memory(state: GraphState):
            return state

        # X√¢y ƒë·ªì th·ªã
        builder.add_node("retrieve", retrieve)
        builder.add_node("generate", generate)
        builder.add_node("update_memory", update_memory)

        builder.set_entry_point("retrieve")
        builder.add_edge("retrieve", "generate")
        builder.add_edge("generate", "update_memory")
        builder.add_edge("update_memory", END)

        # Compile v·ªõi MemorySaver
        return builder.compile(checkpointer=self.memory)

    async def generate_response(self, query: str, session_id: Optional[str] = None) -> Dict:
        if not session_id:
            session_id = str(uuid4())

        # B∆∞·ªõc 1: Ph√¢n t√≠ch v√† vi·∫øt l·∫°i query
        rewrite_result = await self.query_rewriter.analyze_and_rewrite(query)

        # B∆∞·ªõc 2: Ki·ªÉm tra c√≥ th·ªÉ x·ª≠ l√Ω kh√¥ng
        if not rewrite_result["can_process"]:
            return {
                "output": "M√¨nh th·∫•y c√¢u h·ªèi c·ªßa b·∫°n c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ kh√°c nhau. ƒê·ªÉ h·ªó tr·ª£ t·ªët h∆°n, b·∫°n c√≥ th·ªÉ chia th√†nh c√°c c√¢u h·ªèi ri√™ng bi·ªát kh√¥ng? üòä",
                "session_id": session_id,
                "messages": [query, "Kh√¥ng th·ªÉ x·ª≠ l√Ω query ph·ª©c t·∫°p"]
            }

        # B∆∞·ªõc 3: S·ª≠ d·ª•ng query ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i ƒë·ªÉ t√¨m ki·∫øm
        processed_query = rewrite_result["rewritten_query"]
        logger.info(f"Original query: {query}")
        logger.info(f"Rewritten query: {processed_query}")

        # B∆∞·ªõc 4: G·ªçi LangGraph v·ªõi query ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán
        result = await self.graph_app.ainvoke(
            {"messages": [HumanMessage(content=processed_query)]},
            config={"configurable": {"thread_id": session_id}}
        )

        final_answer = result["messages"][-1].content
        return {
            "output": final_answer,
            "session_id": session_id,
            "messages": [msg.content for msg in result["messages"]]
        }






