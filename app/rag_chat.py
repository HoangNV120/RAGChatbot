from typing import Dict, Optional, List, TypedDict, AsyncGenerator
from uuid import uuid4
import logging
import asyncio
import time
from functools import lru_cache

from langchain_core.messages import HumanMessage, BaseMessage
from langchain.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langchain_openai import ChatOpenAI
from app.vector_store import VectorStore
from app.pre_retrieval import PreRetrieval
from app.post_retrieval import PostRetrieval
from app.config import settings
import aiosqlite

# C·∫•u h√¨nh logging v·ªõi level cao h∆°n ƒë·ªÉ gi·∫£m overhead
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# Khai b√°o state cho LangGraph
class GraphState(TypedDict):
    messages: List[BaseMessage]
    docs: Optional[List]
    subqueries: Optional[List[str]]  # Th√™m field cho subqueries (c√¢u h·ªèi ph·ª•)

class RAGChat:
    _db_connection = None  # Shared connection pool
    _llm_instance = None   # Singleton LLM instance

    def __init__(self, vector_store: Optional[VectorStore] = None):
        self.vector_store = vector_store if vector_store else VectorStore()
        self.query_rewriter = PreRetrieval()
        self.post_retrieval = PostRetrieval()  # Kh·ªüi t·∫°o PostRetrieval ƒë·ªÉ √°p d·ª•ng reranking

        self.system_prompt = """B·∫°n l√† *Tr·ª£ l√Ω Sinh vi√™n FPTU* ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn.
**NGUY√äN T·∫ÆC TUY·ªÜT ƒê·ªêI:**
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin c√≥ trong Context ƒë∆∞·ª£c cung c·∫•p
- KH√îNG ƒë∆∞·ª£c th√™m ki·∫øn th·ª©c t·ª´ b√™n ngo√†i context
- KH√îNG ƒë∆∞·ª£c tr·∫£ l·ªùi user query t·ªïng qu√°t (to√°n h·ªçc, khoa h·ªçc, l·∫≠p tr√¨nh)

**QUY T·∫ÆC TR√çCH D·∫™N:**
- N·∫øu context c√≥ ƒë·ªß th√¥ng tin ‚Üí Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n context
- N·∫øu context c√≥ th√¥ng tin m·ªôt ph·∫ßn ‚Üí Tr·∫£ l·ªùi ph·∫ßn c√≥ + "ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt, b·∫°n li√™n h·ªá Ph√≤ng CTSV"
- N·∫øu context kh√¥ng c√≥ th√¥ng tin ‚Üí "M√¨nh ch∆∞a c√≥ d·ªØ li·ªáu, b·∫°n vui l√≤ng li√™n h·ªá Ph√≤ng CTSV nh√©."

**X·ª¨ L√ù C√ÇU H·ªéI ƒê·∫∂C BI·ªÜT:**
- C√¢u h·ªèi Yes/No: Ki·ªÉm tra th√¥ng tin trong context, tr·∫£ l·ªùi "ƒê√∫ng" ho·∫∑c "Kh√¥ng ƒë√∫ng" + gi·∫£i th√≠ch d·ª±a tr√™n context
- C√¢u h·ªèi so s√°nh/x√°c minh: So s√°nh th√¥ng tin trong c√¢u h·ªèi v·ªõi th√¥ng tin trong context, n·∫øu sai th√¨ ƒë∆∞a ra th√¥ng tin ƒë√∫ng t·ª´ context

**ƒê∆Ø·ª¢C PH√âP S·ª¨ D·ª§NG:**
- So s√°nh th√¥ng tin c√≥ trong context
- T·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ph·∫ßn c·ªßa context
- Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c th√¥ng tin trong context
- R√∫t ra k·∫øt lu·∫≠n logic d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn trong context
**C√ÅCH TR·∫¢ L·ªúI:**
- D√πng "b·∫°n/m√¨nh", th√¢n thi·ªán
- Tr√≠ch d·∫´n tr·ª±c ti·∫øp t·ª´ context
- C√≥ th·ªÉ t·ªïng h·ª£p v√† so s√°nh th√¥ng tin trong context
- Kh√¥ng ƒë·∫∑t c√¢u h·ªèi ng∆∞·ª£c l·∫°i
**TUY·ªÜT ƒê·ªêI KH√îNG:**
- S·ª≠ d·ª•ng ki·∫øn th·ª©c t·ªïng qu√°t kh√¥ng c√≥ trong context
- Th√™m th√¥ng tin t·ª´ b√™n ngo√†i context
- Gi·∫£i th√≠ch kh√°i ni·ªám kh√¥ng c√≥ trong context"""

        # S·ª≠ d·ª•ng singleton LLM ƒë·ªÉ tr√°nh t·∫°o m·ªõi nhi·ªÅu l·∫ßn
        if RAGChat._llm_instance is None:
            RAGChat._llm_instance = ChatOpenAI(
                model=settings.model_name,
                temperature=settings.temperature,
                api_key=settings.openai_api_key,
                max_retries=2,  # Gi·∫£m retries ƒë·ªÉ ph·∫£n h·ªìi nhanh h∆°n
                timeout=30,  # Timeout 30s thay v√¨ m·∫∑c ƒë·ªãnh 60s
                streaming=True,
            )
        self.llm = RAGChat._llm_instance

        # Cache prompt template
        self.prompt_template = PromptTemplate.from_template(
            """{system_prompt}

---

Context ƒë∆∞·ª£c cung c·∫•p: 
{context}

---

User query c·∫ßn tr·∫£ l·ªùi: {question}

---

**H∆Ø·ªöNG D·∫™N X·ª¨ L√ù:**
1. ƒê·ªçc k·ªπ Context tr√™n
2. Ki·ªÉm tra xem Context c√≥ ch·ª©a th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi user query kh√¥ng
3. N·∫øu C√ì ‚Üí Tr·∫£ l·ªùi d·ª±a ho√†n to√†n tr√™n th√¥ng tin trong Context
4. N·∫øu KH√îNG ‚Üí Tr·∫£ l·ªùi "M√¨nh ch∆∞a c√≥ d·ªØ li·ªáu, b·∫°n vui l√≤ng li√™n h·ªá Ph√≤ng CTSV nh√©."

**L∆ØU √ù:** Tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c th√™m th√¥ng tin t·ª´ b√™n ngo√†i Context.

Tr·∫£ l·ªùi:"""
        )

        # Kh·ªüi t·∫°o LangGraph v·ªõi memory ƒë·ªÉ l∆∞u l·ªãch s·ª≠ theo thread_id
        self.memory = None
        self.graph_app = None

    @classmethod
    async def _get_shared_db_connection(cls):
        """Singleton database connection ƒë·ªÉ tr√°nh t·∫°o nhi·ªÅu connection"""
        if cls._db_connection is None:
            cls._db_connection = await aiosqlite.connect(
                "chat_sessions.db",
                check_same_thread=False
            )
            # T·ªëi ∆∞u h√≥a SQLite
            await cls._db_connection.execute("PRAGMA journal_mode=WAL")
            await cls._db_connection.execute("PRAGMA synchronous=NORMAL")
            await cls._db_connection.execute("PRAGMA cache_size=10000")
            await cls._db_connection.execute("PRAGMA temp_store=memory")
        return cls._db_connection

    async def _ensure_graph_ready(self):
        """ƒê·∫£m b·∫£o graph ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o"""
        if self.graph_app is None:
            self.graph_app = await self._build_graph()

    async def _get_memory(self):
        """Lazy initialization c·ªßa memory checkpointer v·ªõi shared connection"""
        if self.memory is None:
            conn = await self._get_shared_db_connection()
            self.memory = AsyncSqliteSaver(conn)
        return self.memory

    @lru_cache(maxsize=128)
    def _format_prompt_cached(self, system_prompt: str, context: str, question: str) -> str:
        """Cache prompt formatting ƒë·ªÉ tr√°nh format l·∫°i"""
        return self.prompt_template.format(
            system_prompt=system_prompt,
            context=context,
            question=question
        )

    async def _build_graph(self):
        builder = StateGraph(GraphState)

        # Node: truy xu·∫•t t√†i li·ªáu v·ªõi parallel processing, subqueries v√† reranking
        async def retrieve(state: GraphState):
            question = state["messages"][-1].content

            # L·∫•y subqueries t·ª´ state n·∫øu c√≥, n·∫øu kh√¥ng th√¨ ch·ªâ d√πng c√¢u h·ªèi g·ªëc
            subqueries = state.get('subqueries', [question])

            print(len(subqueries))

            # T·ªëi ∆∞u: gi·∫£m k cho m·ªói query ƒë·ªÉ t·ªïng s·ªë docs kh√¥ng qu√° l·ªõn
            k_per_query = max(1, min(3, 6 // (len(subqueries) - 1)))

            # T√¨m ki·∫øm song song v·ªõi t·∫•t c·∫£ c√°c c√¢u h·ªèi ph·ª•
            search_tasks = []
            for query in subqueries:
                task = asyncio.create_task(
                    self.vector_store.similarity_search(query, k=k_per_query)
                )
                search_tasks.append(task)

            try:
                # Ch·ªù t·∫•t c·∫£ c√°c t√¨m ki·∫øm ho√†n th√†nh v·ªõi timeout
                all_results = await asyncio.wait_for(
                    asyncio.gather(*search_tasks, return_exceptions=True),
                    timeout=20.0
                )

                # K·∫øt h·ª£p v√† lo·∫°i b·ªè tr√πng l·∫∑p
                combined_docs = []
                seen_content = set()

                for results in all_results:
                    if isinstance(results, list):  # Ki·ªÉm tra kh√¥ng ph·∫£i exception
                        for doc in results:
                            # S·ª≠ d·ª•ng hash c·ªßa content ƒë·ªÉ ki·ªÉm tra tr√πng l·∫∑p
                            content_hash = hash(doc.page_content)
                            if content_hash not in seen_content:
                                seen_content.add(content_hash)
                                combined_docs.append(doc)

                # √Åp d·ª•ng LLM-based reranking n·∫øu c√≥ nhi·ªÅu documents
                # if len(combined_docs) > 4:
                #     try:
                #         # S·ª≠ d·ª•ng method llm_rerank public thay v√¨ _llm_rerank private
                #         reranked_docs = await asyncio.wait_for(
                #             self.post_retrieval.llm_rerank(question, combined_docs, top_k=4),
                #             timeout=15.0
                #         )
                #         logger.info(f"LLM-based reranked {len(combined_docs)} docs to top {len(reranked_docs)}")
                #         final_docs = reranked_docs
                #     except asyncio.TimeoutError:
                #         logger.warning("LLM-based reranking timeout, using original docs")
                #         final_docs = combined_docs[:4]
                #     except Exception as e:
                #         logger.warning(f"LLM-based reranking failed: {e}, using original docs")
                #         final_docs = combined_docs[:4]
                # else:
                final_docs = combined_docs

                return {**state, "docs": final_docs}

            except asyncio.TimeoutError:
                logger.warning(f"Vector search timeout for subqueries")
                # Fallback: t√¨m ki·∫øm v·ªõi c√¢u h·ªèi g·ªëc
                try:
                    docs = await asyncio.wait_for(
                        self.vector_store.similarity_search(question, k=4),
                        timeout=10.0
                    )
                    return {**state, "docs": docs}
                except:
                    return {**state, "docs": []}

        # Node: t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM v·ªõi optimization
        async def generate(state: GraphState):
            question = state["messages"][-1].content
            docs = state["docs"]

            # S·ª≠ d·ª•ng to√†n b·ªô context t·ª´ documents (b·ªè gi·ªõi h·∫°n)
            if docs:
                context_parts = []
                for doc in docs:
                    context_parts.append(doc.page_content)
                context = "\n\n".join(context_parts)
            else:
                context = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."

            # S·ª≠ d·ª•ng cached prompt formatting
            prompt = self._format_prompt_cached(
                self.system_prompt,
                context,
                question
            )

            # Ch·ªâ log khi c·∫ßn thi·∫øt (level WARNING)
            if logger.isEnabledFor(logging.INFO):
                logger.info(f"Context length: {len(context)} characters")

            try:
                # Th√™m timeout cho LLM call
                response = await asyncio.wait_for(
                    self.llm.ainvoke([HumanMessage(content=prompt)]),
                    timeout=25.0  # Timeout 25s
                )
            except asyncio.TimeoutError:
                logger.warning("LLM response timeout")
                response = HumanMessage(content="Xin l·ªói, h·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau.")

            return {
                "messages": state["messages"] + [response],
                "docs": None  # X√≥a docs ƒë·ªÉ kh√¥ng l∆∞u v√†o memory
            }

        # X√¢y ƒë·ªì th·ªã
        builder.add_node("retrieve", retrieve)
        builder.add_node("generate", generate)

        builder.set_entry_point("retrieve")
        builder.add_edge("retrieve", "generate")
        builder.add_edge("generate", END)

        # Compile v·ªõi MemorySaver
        memory = await self._get_memory()
        return builder.compile(
            checkpointer=memory,
            interrupt_before=[],
            interrupt_after=[]
        )

    async def generate_response(self, query: str, session_id: Optional[str] = None) -> Dict:
        await self._ensure_graph_ready()

        if not session_id or session_id.strip() == "":
            session_id = str(uuid4())

        # B∆∞·ªõc 1: Ph√¢n t√≠ch v√† vi·∫øt l·∫°i query v·ªõi timeout
        try:
            rewrite_result = await asyncio.wait_for(
                self.query_rewriter.analyze_and_rewrite(query),
                timeout=5.0
            )
        except asyncio.TimeoutError:
            logger.warning("Query rewriter timeout, using original query")
            rewrite_result = {"can_process": True, "rewritten_query": query}
        #
        # # B∆∞·ªõc 2: Ki·ªÉm tra c√≥ th·ªÉ x·ª≠ l√Ω kh√¥ng
        # if not rewrite_result["can_process"]:
        #     return {
        #         "output": "M√¨nh th·∫•y c√¢u h·ªèi c·ªßa b·∫°n c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ kh√°c nhau. ƒê·ªÉ h·ªó tr·ª£ t·ªët h∆°n, b·∫°n c√≥ th·ªÉ chia th√†nh c√°c c√¢u h·ªèi ri√™ng bi·ªát kh√¥ng? üòä",
        #         "session_id": session_id,
        #         "messages": [query, "Kh√¥ng th·ªÉ x·ª≠ l√Ω query ph·ª©c t·∫°p"]
        #     }

        processed_query = rewrite_result["rewritten_query"]

        # B∆∞·ªõc 3: L·∫•y l·ªãch s·ª≠ chat t·ª´ memory v·ªõi optimization
        config = {"configurable": {"thread_id": session_id}}

        try:
            # Timeout cho vi·ªác l·∫•y state
            current_state = await asyncio.wait_for(
                self.graph_app.aget_state(config),
                timeout=3.0
            )
            existing_messages = current_state.values.get("messages", []) if current_state.values else []
        except (asyncio.TimeoutError, Exception):
            existing_messages = []

        # Th√™m message m·ªõi v√† gi·ªõi h·∫°n history
        all_messages = existing_messages + [HumanMessage(content=processed_query)]

        # Ch·ªâ l·∫•y 3 messages g·∫ßn nh·∫•t thay v√¨ 5 ƒë·ªÉ gi·∫£m context length
        recent_messages = all_messages[-3:] if len(all_messages) > 3 else all_messages

        # B∆∞·ªõc 4: Chu·∫©n b·ªã state v·ªõi subqueries
        initial_state = {
            "messages": recent_messages,
            "subqueries": rewrite_result.get("subqueries", [processed_query])
        }

        # B∆∞·ªõc 5: G·ªçi LangGraph v·ªõi timeout
        try:
            result = await asyncio.wait_for(
                self.graph_app.ainvoke(initial_state, config=config),
                timeout=40.0  # T·ªïng timeout 40s
            )
            final_answer = result["messages"][-1].content
        except asyncio.TimeoutError:
            logger.warning("Graph execution timeout")
            final_answer = "Xin l·ªói, h·ªá th·ªëng ƒëang x·ª≠ l√Ω ch·∫≠m. Vui l√≤ng th·ª≠ l·∫°i sau."
            result = {"messages": recent_messages + [HumanMessage(content=final_answer)]}

        return {
            "output": final_answer,
            "session_id": session_id,
            "messages": [msg.content for msg in result["messages"]],
            "subqueries": rewrite_result.get("subqueries", [processed_query])  # Th√™m th√¥ng tin debug v·ªÅ c√°c c√¢u h·ªèi ph·ª•
        }

    async def generate_response_stream(self, query: str, session_id: Optional[str] = None) -> AsyncGenerator[Dict, None]:
        """
        Streaming version s·ª≠ d·ª•ng LangGraph astream
        """
        await self._ensure_graph_ready()

        if not session_id or session_id.strip() == "":
            session_id = str(uuid4())

        try:
            # B∆∞·ªõc 1: Ph√¢n t√≠ch v√† vi·∫øt l·∫°i query
            try:
                rewrite_result = await asyncio.wait_for(
                    self.query_rewriter.analyze_and_rewrite(query),
                    timeout=5.0
                )
            except asyncio.TimeoutError:
                logger.warning("Query rewriter timeout, using original query")
                rewrite_result = {"can_process": True, "rewritten_query": query}

            processed_query = rewrite_result["rewritten_query"]

            # B∆∞·ªõc 2: L·∫•y l·ªãch s·ª≠ chat t·ª´ memory
            config = {"configurable": {"thread_id": session_id}}

            try:
                current_state = await asyncio.wait_for(
                    self.graph_app.aget_state(config),
                    timeout=3.0
                )
                existing_messages = current_state.values.get("messages", []) if current_state.values else []
            except (asyncio.TimeoutError, Exception):
                existing_messages = []

            # Th√™m message m·ªõi v√† gi·ªõi h·∫°n history
            all_messages = existing_messages + [HumanMessage(content=processed_query)]
            recent_messages = all_messages[-3:] if len(all_messages) > 3 else all_messages

            # B∆∞·ªõc 3: Chu·∫©n b·ªã state v·ªõi subqueries
            initial_state = {
                "messages": recent_messages,
                "subqueries": rewrite_result.get("subqueries", [processed_query])
            }

            # B∆∞·ªõc 4: S·ª≠ d·ª•ng LangGraph astream
            full_response = ""
            async for chunk in self.graph_app.astream(initial_state, config=config, stream_mode="updates"):
                # X·ª≠ l√Ω chunk t·ª´ retrieve node
                if "retrieve" in chunk:
                    # C√≥ th·ªÉ yield th√¥ng tin v·ªÅ vi·ªác ƒëang t√¨m ki·∫øm
                    continue

                # X·ª≠ l√Ω chunk t·ª´ generate node
                if "generate" in chunk:
                    state = chunk["generate"]
                    if "messages" in state and state["messages"]:
                        last_message = state["messages"][-1]
                        if hasattr(last_message, 'content'):
                            # N·∫øu l√† streaming response t·ª´ LLM
                            if hasattr(last_message, 'response_metadata') and last_message.response_metadata.get('streaming', False):
                                full_response += last_message.content
                                yield {
                                    "type": "chunk",
                                    "content": last_message.content,
                                    "timestamp": time.time()
                                }
                            else:
                                # N·∫øu l√† complete response, stream theo t·ª´ng chunk
                                response_content = last_message.content
                                if response_content != full_response:
                                    new_content = response_content[len(full_response):]
                                    full_response = response_content

                                    # Stream t·ª´ng t·ª´ ƒë·ªÉ c√≥ hi·ªáu ·ª©ng typing
                                    words = new_content.split()
                                    for word in words:
                                        yield {
                                            "type": "chunk",
                                            "content": word + " ",
                                            "timestamp": time.time()
                                        }
                                        await asyncio.sleep(0.01)  # Small delay

            # Yield done event
            yield {
                "type": "done",
                "session_id": session_id,
                "route_used": "RAG_CHAT",
                "routing_info": {},
                "timestamp": time.time(),
                "subqueries": rewrite_result.get("subqueries", [processed_query]),
                "final_answer": full_response
            }

        except Exception as e:
            logger.error(f"Error in RAG streaming: {e}")
            yield {
                "type": "error",
                "content": "ü§ñ Xin l·ªói, c√≥ l·ªói x·∫£y ra. B·∫°n vui l√≤ng th·ª≠ l·∫°i sau.",
                "timestamp": time.time()
            }

