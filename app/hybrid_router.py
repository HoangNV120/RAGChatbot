from typing import Dict, Optional, List, Tuple
import logging
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from app.config import settings
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import asyncio

logger = logging.getLogger(__name__)

class HybridRouter:
    """
    Hybrid Router káº¿t há»£p LLM Classification + Vector Similarity
    
    Luá»“ng hoáº¡t Ä‘á»™ng:
    1. LLM Classification Ä‘á»ƒ xÃ¡c Ä‘á»‹nh category
    2. Náº¿u category = "KHÃC" â†’ RAG_CHAT (bá» qua vector)
    3. Náº¿u category thuá»™c cÃ¡c category Ä‘á»‹nh trÆ°á»›c â†’ Vector Router
    4. Vector Router: tÃ¬m similarity, náº¿u Ä‘á»§ cao â†’ tráº£ answer, náº¿u khÃ´ng â†’ RAG_CHAT
    """
    
    def __init__(self):
        # Initialize LLM cho classification
        self.llm = ChatOpenAI(
            openai_api_key=settings.openai_api_key,
            model_name="gpt-3.5-turbo",
            temperature=0
        )
        
        # Initialize embeddings cho vector search
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.openai_api_key,
            model="text-embedding-ada-002"
        )
        
        # Load data tá»« data_test.xlsx
        self.data = pd.read_excel('app/data_test.xlsx')
        
        # Similarity threshold cho vector search
        self.similarity_threshold = 0.75
        
        # Cache cho embeddings
        self.question_embeddings = None
        self.questions_data = []
        
        # Äá»‹nh nghÄ©a cÃ¡c category há»£p lá»‡
        self.valid_categories = [
            "Há»ŒC PHÃ", "NGÃ€NH Há»ŒC", "QUY CHáº¾ THI", "ÄIá»‚M Sá»", 
            "Dá»ŠCH Vá»¤ SINH VIÃŠN", "CÆ  Sá» Váº¬T CHáº¤T", "CHÆ¯Æ NG TRÃŒNH Há»ŒC"
        ]
        
        print(f"HybridRouter initialized with {len(self.data)} questions")
        print(f"Valid categories: {self.valid_categories}")
        
    async def _classify_query_category(self, query: str) -> Dict:
        """BÆ°á»›c 1: PhÃ¢n loáº¡i category báº±ng LLM"""
        
        system_prompt = f"""Báº¡n lÃ  má»™t há»‡ thá»‘ng phÃ¢n loáº¡i cÃ¢u há»i vá» trÆ°á»ng Ä‘áº¡i há»c. HÃ£y phÃ¢n loáº¡i cÃ¢u há»i vÃ o má»™t trong cÃ¡c danh má»¥c sau:

DANH Má»¤C Há»¢P Lá»†:
- Há»ŒC PHÃ: CÃ¢u há»i vá» chi phÃ­ há»c táº­p, há»c phÃ­, miá»…n giáº£m há»c phÃ­
- NGÃ€NH Há»ŒC: CÃ¢u há»i vá» cÃ¡c ngÃ nh há»c, chuyÃªn ngÃ nh, chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o
- QUY CHáº¾ THI: CÃ¢u há»i vá» quy Ä‘á»‹nh thi cá»­, Ä‘iá»u kiá»‡n thi, lá»‹ch thi
- ÄIá»‚M Sá»: CÃ¢u há»i vá» Ä‘iá»ƒm sá»‘, thang Ä‘iá»ƒm, cÃ¡ch tÃ­nh Ä‘iá»ƒm
- Dá»ŠCH Vá»¤ SINH VIÃŠN: CÃ¢u há»i vá» dá»‹ch vá»¥ há»— trá»£ sinh viÃªn, thá»§ tá»¥c hÃ nh chÃ­nh
- CÆ  Sá» Váº¬T CHáº¤T: CÃ¢u há»i vá» cÆ¡ sá»Ÿ váº­t cháº¥t, phÃ²ng há»c, thÆ° viá»‡n, kÃ½ tÃºc xÃ¡
- CHÆ¯Æ NG TRÃŒNH Há»ŒC: CÃ¢u há»i vá» chÆ°Æ¡ng trÃ¬nh há»c, mÃ´n há»c, thá»i khÃ³a biá»ƒu

QUAN TRá»ŒNG:
- Náº¿u cÃ¢u há»i KHÃ”NG thuá»™c cÃ¡c danh má»¥c trÃªn hoáº·c khÃ´ng liÃªn quan Ä‘áº¿n trÆ°á»ng Ä‘áº¡i há»c, hÃ£y tráº£ vá» "KHÃC"
- Chá»‰ tráº£ vá» TÃŠN DANH Má»¤C, khÃ´ng giáº£i thÃ­ch thÃªm
- VÃ­ dá»¥ cÃ¢u há»i thuá»™c "KHÃC": thá»i tiáº¿t, náº¥u Äƒn, thá»ƒ thao, tin tá»©c, cÃ´ng nghá»‡ khÃ´ng liÃªn quan giÃ¡o dá»¥c...

CHá»ˆ TRáº¢ Vá»€ TÃŠN DANH Má»¤C DUY NHáº¤T."""

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"CÃ¢u há»i: {query}")
            ]
            
            response = await self.llm.agenerate([messages])
            category = response.generations[0][0].text.strip().upper()
            
            # Kiá»ƒm tra category cÃ³ há»£p lá»‡ khÃ´ng
            if category in self.valid_categories:
                print(f"ğŸ·ï¸  Category classified: {category}")
                return {
                    "category": category,
                    "is_valid": True,
                    "should_use_vector": True
                }
            else:
                print(f"ğŸ·ï¸  Category classified: {category} (INVALID - treating as KHÃC)")
                return {
                    "category": "KHÃC",
                    "is_valid": False,
                    "should_use_vector": False
                }
                
        except Exception as e:
            logger.error(f"Error in LLM classification: {e}")
            print(f"âŒ LLM classification error: {e}")
            # Fallback: coi nhÆ° category KHÃC
            return {
                "category": "KHÃC",
                "is_valid": False,
                "should_use_vector": False,
                "error": str(e)
            }
    
    async def _initialize_embeddings(self):
        """Khá»Ÿi táº¡o embeddings cho vector search (giá»‘ng VectorRouter)"""
        
        if self.question_embeddings is not None:
            return  # ÄÃ£ khá»Ÿi táº¡o rá»“i
            
        print("ğŸ”„ Initializing question embeddings for vector search...")
        
        # Chuáº©n bá»‹ danh sÃ¡ch cÃ¢u há»i
        questions = []
        self.questions_data = []
        
        for idx, row in self.data.iterrows():
            question = str(row['question']).strip()
            answer = str(row['answer']).strip()
            source = str(row.get('nguá»“n', '')).strip()
            
            if question and answer:  # Chá»‰ láº¥y cÃ¢u há»i cÃ³ answer
                # Xá»­ lÃ½ multiple questions (náº¿u cÃ³ dáº¥u |)
                question_parts = [q.strip() for q in question.split('|') if q.strip()]
                
                for q in question_parts:
                    questions.append(q)
                    self.questions_data.append({
                        'original_index': idx,
                        'question': q,
                        'answer': answer,
                        'source': source
                    })
        
        print(f"ğŸ“Š Processing {len(questions)} questions for embedding...")
        
        # Táº¡o embeddings cho táº¥t cáº£ cÃ¢u há»i
        try:
            # Batch processing Ä‘á»ƒ tá»‘i Æ°u
            batch_size = 50
            all_embeddings = []
            
            for i in range(0, len(questions), batch_size):
                batch = questions[i:i + batch_size]
                print(f"   Processing batch {i//batch_size + 1}/{(len(questions) + batch_size - 1)//batch_size}")
                
                # Get embeddings for batch
                batch_embeddings = await self.embeddings.aembed_documents(batch)
                all_embeddings.extend(batch_embeddings)
                
                # Small delay to avoid rate limiting
                await asyncio.sleep(0.1)
            
            # Convert to numpy array
            self.question_embeddings = np.array(all_embeddings)
            
            print(f"âœ… Successfully created embeddings for {len(questions)} questions")
            print(f"ğŸ“ Embedding dimension: {self.question_embeddings.shape[1]}")
            
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            print(f"âŒ Error creating embeddings: {e}")
            raise
    
    async def _get_query_embedding(self, query: str) -> np.ndarray:
        """Láº¥y embedding cho cÃ¢u há»i input"""
        try:
            embedding = await self.embeddings.aembed_query(query)
            return np.array(embedding)
        except Exception as e:
            logger.error(f"Error getting query embedding: {e}")
            raise
    
    def _find_most_similar_question(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[int, float, Dict]]:
        """TÃ¬m cÃ¢u há»i tÆ°Æ¡ng tá»± nháº¥t trong database"""
        
        if self.question_embeddings is None:
            return []
        
        # TÃ­nh cosine similarity
        similarities = cosine_similarity([query_embedding], self.question_embeddings)[0]
        
        # Láº¥y top_k cÃ¢u há»i cÃ³ similarity cao nháº¥t
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            similarity_score = similarities[idx]
            question_data = self.questions_data[idx]
            
            results.append((idx, similarity_score, question_data))
        
        return results
    
    async def _vector_search(self, query: str) -> Dict:
        """BÆ°á»›c 3: Vector search (giá»‘ng VectorRouter)"""
        
        # Khá»Ÿi táº¡o embeddings náº¿u chÆ°a cÃ³
        await self._initialize_embeddings()
        
        print(f"ğŸ” Vector search for query: {query[:50]}...")
        
        # Láº¥y embedding cho cÃ¢u há»i input
        query_embedding = await self._get_query_embedding(query)
        
        # TÃ¬m cÃ¢u há»i tÆ°Æ¡ng tá»± nháº¥t
        similar_questions = self._find_most_similar_question(query_embedding, top_k=3)
        
        if not similar_questions:
            print("âŒ No similar questions found in vector search")
            return {
                "route": "RAG_CHAT",
                "reason": "No questions in database",
                "similarity_score": 0.0
            }
        
        # Láº¥y cÃ¢u há»i cÃ³ similarity cao nháº¥t
        best_idx, best_similarity, best_question_data = similar_questions[0]
        
        print(f"ğŸ“Š Best vector similarity: {best_similarity:.3f}")
        print(f"ğŸ” Best match: {best_question_data['question'][:50]}...")
        
        # Quyáº¿t Ä‘á»‹nh dá»±a trÃªn threshold
        if best_similarity >= self.similarity_threshold:
            print(f"âœ… Found good vector match (similarity: {best_similarity:.3f} >= {self.similarity_threshold})")
            
            return {
                "route": "VECTOR_BASED",
                "similarity_score": best_similarity,
                "matched_question": best_question_data['question'],
                "answer": best_question_data['answer'],
                "source": best_question_data['source'],
                "all_matches": [
                    {
                        "question": q_data['question'],
                        "similarity": sim,
                        "answer": q_data['answer'][:100] + "..." if len(q_data['answer']) > 100 else q_data['answer']
                    }
                    for _, sim, q_data in similar_questions
                ]
            }
        else:
            print(f"âŒ Vector similarity too low (best: {best_similarity:.3f} < {self.similarity_threshold})")
            
            return {
                "route": "RAG_CHAT",
                "reason": f"Vector similarity {best_similarity:.3f} below threshold {self.similarity_threshold}",
                "similarity_score": best_similarity,
                "best_match": {
                    "question": best_question_data['question'],
                    "similarity": best_similarity,
                    "answer": best_question_data['answer'][:100] + "..." if len(best_question_data['answer']) > 100 else best_question_data['answer']
                }
            }
    
    async def route_query(self, query: str) -> Dict:
        """Main routing function vá»›i hybrid approach"""
        try:
            print(f"\nğŸš€ Hybrid routing for query: {query[:50]}...")
            
            # BÆ°á»›c 1: LLM Classification
            classification_result = await self._classify_query_category(query)
            category = classification_result["category"]
            should_use_vector = classification_result["should_use_vector"]
            
            # BÆ°á»›c 2: Quyáº¿t Ä‘á»‹nh luá»“ng dá»±a trÃªn category
            if not should_use_vector or category == "KHÃC":
                print(f"âš¡ Category '{category}' â†’ Skip vector search â†’ Direct RAG_CHAT")
                return {
                    "route": "RAG_CHAT",
                    "reason": f"Category '{category}' requires full RAG processing",
                    "query": query,
                    "classification": classification_result,
                    "similarity_score": 0.0
                }
            
            # BÆ°á»›c 3: Vector Search cho category há»£p lá»‡
            print(f"ğŸ¯ Category '{category}' â†’ Proceed to vector search...")
            vector_result = await self._vector_search(query)
            
            # ThÃªm thÃ´ng tin classification vÃ o káº¿t quáº£
            vector_result["query"] = query
            vector_result["classification"] = classification_result
            
            return vector_result
            
        except Exception as e:
            logger.error(f"Error in hybrid routing: {e}")
            print(f"âŒ Error in hybrid routing: {e}")
            return {
                "route": "RAG_CHAT",
                "reason": f"Error during hybrid routing: {str(e)}",
                "query": query,
                "similarity_score": 0.0
            }
    
    def get_stats(self) -> Dict:
        """Thá»‘ng kÃª vá» hybrid router"""
        stats = {
            "total_questions": len(self.questions_data),
            "embedding_dimension": self.question_embeddings.shape[1] if self.question_embeddings is not None else 0,
            "similarity_threshold": self.similarity_threshold,
            "valid_categories": self.valid_categories,
            "vector_status": "ready" if self.question_embeddings is not None else "not_initialized"
        }
        
        return stats
    
    def set_similarity_threshold(self, threshold: float):
        """Äiá»u chá»‰nh threshold cho vector search"""
        if 0.0 <= threshold <= 1.0:
            self.similarity_threshold = threshold
            print(f"ğŸ“Š Vector similarity threshold updated to: {threshold}")
        else:
            print(f"âŒ Invalid threshold: {threshold}. Must be between 0.0 and 1.0")
    
    async def test_query(self, query: str, show_details: bool = True) -> Dict:
        """Test má»™t cÃ¢u há»i vÃ  hiá»ƒn thá»‹ chi tiáº¿t toÃ n bá»™ luá»“ng"""
        
        print(f"\nğŸ§ª Testing hybrid routing for: '{query}'")
        
        # Test classification
        print(f"\nğŸ“‹ Step 1: LLM Classification")
        classification_result = await self._classify_query_category(query)
        print(f"   Category: {classification_result['category']}")
        print(f"   Should use vector: {classification_result['should_use_vector']}")
        
        if classification_result['should_use_vector']:
            print(f"\nğŸ” Step 2: Vector Search")
            await self._initialize_embeddings()
            
            # Get embedding
            query_embedding = await self._get_query_embedding(query)
            
            # Find similar questions
            similar_questions = self._find_most_similar_question(query_embedding, top_k=5)
            
            if show_details:
                print(f"\nğŸ“Š Top 5 similar questions:")
                for i, (idx, similarity, q_data) in enumerate(similar_questions, 1):
                    print(f"  {i}. Similarity: {similarity:.3f}")
                    print(f"     Question: {q_data['question']}")
                    print(f"     Answer: {q_data['answer'][:100]}...")
                    print()
        else:
            print(f"\nâš¡ Step 2: Skipped vector search (Category: {classification_result['category']})")
        
        # Get final routing result
        print(f"\nğŸ¯ Final Routing Result:")
        result = await self.route_query(query)
        
        print(f"   Route: {result['route']}")
        if result['route'] == "VECTOR_BASED":
            print(f"   âœ… Matched answer: {result['answer'][:150]}...")
            print(f"   ğŸ“Š Similarity: {result['similarity_score']:.3f}")
        else:
            print(f"   âŒ Reason: {result['reason']}")
        
        return result
